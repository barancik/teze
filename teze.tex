\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[utf8]{inputenc} %http://depling.org/depling2015/formats/depling2015latex/depling2015latex.zip
\usepackage{color}
\usepackage{xspace}
\usepackage{url}

\usepackage{amsmath}
%%\usepackage{color}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}


\def \xxx#1{\textbf{\textcolor{red}{xxx: #1}}}
\def \todo#1{\textbf{\textcolor{red}{todo: #1}}}
\def\equo#1{``#1''}

\def\Tref#1{Table~\ref{#1}}
\def\Fref#1{Figure~\ref{#1}}
\def\Sref#1{Section~\ref{#1}}
\newcommand{\out}[1]{\textcolor[rgb]{0.8,0.8,0.8}{\textbf{#1}}}
\newcommand{\ml}[1]{\textcolor{red}{\raisebox{.2ex}{\tiny ML:~}#1}}
\def\footurl#1{\footnote{\url{#1}}}

%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\title{Targeted Paraphrasing for Machine Translation Evaluation}

\author{Petra Barančíková \\
  Institute of Formal and Applied Linguistics \\
  Charles University in Prague, Faculty of Mathematics and Physics\\
  Malostranské náměstí 25, Prague, Czech Republic \\
  {\tt barancikova@ufal.mff.cuni.cz} \\}

\date{}
%
%This paper describes Parmesan, our submission to the 2014 Workshop on Statistical Machine Translation (WMT) metrics task for evaluation English-to-Czech translation. 
%We show that the Czech Meteor Paraphrase tables are so noisy that they actually can harm the performance of the metric. However, they can be very useful after extensive filtering in targeted paraphrasing of Czech reference sentences prior to the evaluation.
%Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences. It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data. However, this result was not confirmed by this year's data.

\begin{document}

\maketitle
\begin{abstract}
This thesis is concentrated on improving an accuracy of machine translation to 
Czech language using targeted paraphrasing. We develop system that creates new
synthetic reference that are closer in wording to a corresponding machine 
translations. 

Using these new reference sentences makes evaluation using traditional metrics 
is more reliable as they lack of paraphrase and free word order support.

We believe that after implementation, the system for generating new synthetic
references will be very beneficial not only for MT evaluation, but mainly
inside MT systems for improved tuning and development of the systems. This way,
it will directly influence the quality of machine translation itself.

%Trade-off between simplicity and quality of MT metric!!
\end{abstract}

\section{Introduction}
Since the very first appearance of machine translation (MT) systems, a 
necessity for their objective evaluation and comparison arose. The traditional
human evaluation while being the most reliable has serious drawbacks -- it is 
time-consuming and expensive.
%, although there were recently successful attempts to overcome this problem 
%using crowdsourcing techniques. [\markcite{{\it Callison-Burch}, 2009]} and 
%[\markcite{{\it Bentivogli et al.}, 2011]} demonstrate that the average score of 
%high number of non-expert low-paid annotators shows high agreement with gold 
%standards provided by expert annotators in various natural language processing 
%(NLP) tasks including machine translation evaluation.

However, the main problem of human evaluation is that it is highly dependent on 
the person annotating and there is generally very low agreement between several 
human annotators. %footnote - example, only blah blah correlation during \cite{wmt12}.
Moreover, it is practically impossible to repeat the evaluation with the same
results (TODO: nejaka citace??).
%[Bojar - kniha] %nizka shoda pro cestinu?!! Ani ne kniha, ale spis wmt13, co 14?, 
% kde to byla nejak hrozive nizka shoda..

Due to being slow and unreproducible, it is impossible to use human evaluation
for tuning and development of MT systems. Well-performing automatic MT 
evaluation metrics are essential precisely for these tasks.

%Pioneer measures such as BLEU and NIST measure MT quality cheaply and objectively
%through the strong correlation between human judgement and the n-gram overlap
%betweeen  aa system translation and one or more reference translation. 

%The first metric correlating well with human judgment was BLEU \cite{bleu}.
%Due to its simplicity and language-independence, it still remains the most 
%common metric for MT evaluation, even though other, better-performing metrics 
%exist. \cite{wmt14} 

The first metric correlating well with human judgment was BLEU. \cite{bleu}
BLEU is computed from an word sequences overlap between the translated sentence 
(hypothesis) and one or more corresponding reference sentences, i.e., 
translations made by a human translator.

Due to its simplicity and language independence, BLEU still remains the most 
common metric for MT evaluation and tuning, even though other, 
better-performing metrics exist (\cite{wmt13-metrics}, \cite{wmt14}) and 
\cite{callison2006re} shows its correlation with human judgment is not as high 
as previously thought.

%Due to its simplicity and language independence, BLEU remains widely used, even 
%though its correlation with human judgment is not as high as previously thought
%(\cite{callison2006re},\cite{koehn-monz:2006:WMT}??). This is particular valid 
%at a sentence level (Blatz et al. 2003).

Furthermore, obtaining reference sentences is labour intensive and expensive.
At the Linguistic Data Consortium, for example, production of reference 
translation is complicated process involving translation by professional 
agencies based on elaborate guidelines and detailed quality control. 
\cite{strassel} 

%For example, producing reference translation at the Linguistic Data Consortium, a common 
%source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines and quality control processes

Therefore, the standard practice is using only one reference sentence and BLEU 
then tends to perform badly. As there are many translations of a single 
sentence, even a perfectly correct machine translation might get a low score 
due different wording and disregarding synonymous expressions (see 
\Fref{example_of_BLEU_malfunction}). This is especially valid for morphologically 
rich languages with free word order like the Czech language. 
\cite{bojar-tackling-sparse-data}


\begin{figure*}[tb]
\begin{center}
\begin{tabular}{ll}
 Original sentence &  \begin{tabular}{l}
  	\textit{Banks are testing payment by mobile telephone} \\
	\end{tabular}  \\
 \hline
 
 MT output & \begin{tabular}{llllll}
 			\textit{Banky} & \textit{zkou\v sej\'i} & \textit{platbu} & \textit{pomoc\'i} & \textit{mobiln\'iho} & \textit{telefonu} \\
 			Banks & are testing & payment & with help & mobile & phone \\
			\end{tabular} \\
 & \begin{tabular}{l}
  	Banks are testing payment by mobile phone \\
	\end{tabular} \\

% MT output & \textit{Banky zkou\v sej\'i platbu pomoc\'i mobiln\'iho telefonu} \\
% 					 & Banks testing payment help mobile phone \\
% 					 & Banks are testing payment by mobile telephone \\
 \hline
 Reference sentence & \begin{tabular}{llll}
 			\textit{Banky} & \textit{testuj\'i} & \textit{placen\'i} & \textit{mobilem} \\
 			Banks & are testing & paying & by mobile phone \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	Banks are testing paying by mobile phone \\
	\end{tabular}
 
\end{tabular}
\caption{Example from WMT12 - Even though the translation is grammatically 
correct and the meaning of both sentences is very similar, it doesn't contribute 
to the BLEU score. There is only one unigram overlapping.}
\end{center}
\label{example_of_BLEU_malfunction}
\end{figure*}

As the main task of an MT metric is essentially to decide whether the reference
sentences is a paraphrase of the MT output, our goal is to achieve higher 
accuracy of MT evaluation by targeted paraphrasing of reference sentences., i.e. 
creating a new synthetic reference sentence that is still correct and keeps the 
meaning of the original sentence, but at the same time it is closer in wording 
to the MT output. BLEU and other string-based metrics should perform more 
reliable using these new references. (TODO: nejak preformulovat)

The structure of the paper is as follows: in the next section, we present other
work in targeted paraphrasing for MT evaluation. In \Sref{Data}, we introduced
the data we use for our experiments.  In sections \ref{lrec} and \ref{MT}, 
we show paraphrasing based on phrase substitutions and machine translation 
itself. Finally, we conclude with avenues for further work.

\section{Related Work}
There are two attitudes towards solving the problem - one is to change the 
automatic metric itself to be tolerant to other sentence representation and the
other one is to pre-process automatically the reference sentence to new 
synthetic reference that is closer in wording to the original reference 
sentence and keeps its meaning and grammatical correctness.

\subsection{Automatic Metrics}
%Second generation metrics 
Meteor \cite{meteor-wmt:2014}, TERp \cite{terp} and ParaEval \cite{paraeval2} 
also largely focuses on a n-gram overlap while including other linguistically 
motivated resources. They utilize paraphrase support using their own paraphrase 
tables, which they all had acquired automatically using the pivot-based method. 
\cite{pivoting} 

%The Meteor metric outperforms traditional metrics as it explicitly addresses 
%their weaknesses -- it does not only support match on several levels -  exact, 
%stem, synonym, and paraphrase but it also takes into account recall, 
%distinguishes between functional and content words, allows language-specific 
%tuning of parameters and many others.

Meteor paraphrase table are available not only for English, but for other five 
languages, including the Czech language. The basic setting of Meteor for 
evaluation of~Czech sentences offers two levels of matches -- exact and 
paraphrase. As we show further (see \Sref{parmesan}), its paraphrase tables 
are so noisy that they actually harm the performance of the metric, as it can
award mistranslated and even untranslated words.

There is growing belief in the MT community that surface level matching is not
discriminative enough to reflect the translation quality of today’s MT systems %prefomulovat, vykradeno
and employ rich linguistic features that better reflect human perception. (e.g.
 \newcite{liu:2005}, \newcite{owczarzak:2007}, \newcite{amigo:2009}, 
 \newcite{Popovic:2009}, \cite{Pado09},...) 
 
% \cite{Pado09} - as a translation should have the same meaning as the 
%reference, Padó proposes metric based on textual entailment.
 
These metrics shows better correlation with human judgment, but their wide 
usage is limited by being complex and language-dependent. As a result, there is
a trade-off between linguistic-rich strategy for better performance and 
applicability of simple string level matching.

%the development of automatic MT evaluation metrics falls into  a  dilemma of choosing 
%the string level matching for wide application or selecting the linguistic-rich strategy
%for better performance.  Faced with an evaluation task involving multiple  languages as 
%in WMT campaigns [Callison-Burch   et al., 2008; 2009; 2010], a language independent
% metric of high performance is more desirable.

\subsection{Sentence paraphrasing}
% Mozna tu zminit, ze tech metod parafrazovani je hromada (viz Lin a Pantel)
% krome pri ev
Targeted paraphrasing for MT evaluation is introduced in~\newcite{kauchak}. 
They focus on lexical substitution in Chinese-to-English translations. They 
select all pairs of words for which one word appears in a reference sentence, 
second word in a hypothesis, but none of them in both. They keep only 
pairs of synonymous words, i.e. words appearing in the same WordNet 
\cite{wordnet} synset. Each such a pair of words was further contextually 
evaluated. For every confirmed synonym, a new reference sentence is created by 
placing it to the reference sentence on the position of its synonym.

This solution is not quite possible to apply to the Czech language. As Czech 
belongs among~inflective languages with rich morphology, a Czech word has 
typically many forms and the correct form depends heavily on its context, 
e.g., cases of nouns depend on verb valency frames. Changing a single word may 
result into not grammatical sentence. Therefore, we do not attempt to~change 
a~single word in~a~reference sentence but we focus on~creating one single 
correct reference sentence.

There is many other application for paraphrases in the MT field. Paraphrases 
helps translate out-of-vocabulary words and increase quality of MT systems 
trained on sparse data \cite{Callison-Burch:2006}, \cite{Marton:2009} or 
enlarge training data. \cite{nakov2008improved}
\newcite{madnani2010circle} and \newcite{mehay2012shallow} show that single 
translation augmented by targeted paraphrases can successfully replace up to
additional 4 human reference translation.
 
%\xxx{Wubben}

\section{Data}
\label{Data}
\subsection{Text Data}
We perform our experiments on data sets from the English-to-Czech translation 
task of WMT12 \cite{wmt12}, WMT13 \cite{wmt13} and WMT14 \cite{wmt14}. The data
sets contain 13/14\footnote{We use only 12 of them because two of them 
(FDA.2878 and online-G) have no human judgments.}/10 files with Czech outputs 
of MT systems.

In addition, each data set contains one file with corresponding reference 
sentences and one with original English source sentences. We perform 
morphological analysis and tagging of the hypotheses and the reference 
sentences using Morče \cite{morce:2007}.

The human judgment of hypotheses is available as a relative ranking of 
performance of five systems for~a~sentence. We calculated the sbsolute 
performance of every system by the “$ > $ others” method \cite{bojar-grains}, 
which was the WMT12 official system score. It is computed as 
$ \frac{wins}{wins+loses} $, ties among several systems are ignored. We use 
this score as a human judgment in further evaluation.
%We refer to this interpretation of human judgment as 
%\textit{silver standard} to distinguish it from the official system scores, which were 
%computed differently each year (here referred to as \textit{gold standard}).

\subsection{Sources of Paraphrases}
\label{meteori}
We make use of two existing sources of Czech paraphrases -- the Czech WordNet 
1.9 PDT \cite{czech-wordnet}, the Meteor Paraphrase Tables \cite{meteor-tables}. 
Czech PPDB \cite{ppdb} became available only recently and we plan to use it in 
further research.

The \textbf{Czech WordNet 1.9 PDT} is derived from the WordNet \cite{wordnet} 
by automatic translation followed by manual verification. It~contains rather 
high quality lemmatized paraphrases. However, their amount is~insufficient for 
our purposes - it contains only 13k pairs of synonymous lemmas. % and 
%- only one paraphrase per four sentences on~average is found in~the data. % (see \Tref{number_of_substitutions}).  

On~the other hand, Czech \textbf{Meteor Paraphrase tables} are quite the 
opposite of Czech WordNet -- they are large in size, but contain a lot of 
noise, as they were constructed automatically via \textit{pivoting}. 
\cite{pivoting} The pivot method is an inexpensive way of acquiring paraphrases 
from large parallel corpora. It is based on the assumption that two phrases 
that share a meaning may have a same translation in a foreign language. 
\cite{dyvik}

The noise is particularly high among the multiword paraphrases -- for 
example: \textit{svého názoru} (its opinion) and \textit{šermovat rukama a 
mlátit neviditelného} (to flail one's arms and to beat the invisible one) are 
selected as a paraphrase. We show that the amount of noise in the multi-word 
paraphrases is so high that no automatic filtering method we used outperforms 
omitting them completely. (see Appendix - \ref{multiword_filtering}) 
TODO: mozna uplne vynechat a odkazat jen na LRECovsky clanek??
%Mozna je teda uplne vynechat i z toho lrecovskyho experimentu
 
Among one-word paraphrases the noise is sparser, but there are still pairs like 
\textit{1873} - \textit{pijavice} (a leech) or \textit{afgh\'{a}nci} (Afghans) -
\textit{š\v{t}astně} (happily) identified as synonyms. However, the biggest 
problem is that most of synonymous pairs were just different word forms of the 
same lemma. 

We performed automatically filtering among the one-word paraphrases in the 
Meteor table in the following way. Using Morče, we first perform morphological 
analysis of all one-word pairs and replace the word forms with their lemmas. We 
keep only pairs of different lemmas. Further, we dispose of pairs of words that 
differ in their parts of speech (POS)\footnote{With a single exception -- 
paraphrases consisting of numeral and corresponding digits, e.g., 
\textit{osmnáct} (eighteen) and \textit{18} - \textit{osmnáct} has the part of 
speech \textit{C}, which is designated for numerals, \textit{18} is marked with 
\textit{X} meaning it is an unknown word for the morphological analyzer.} or
~contain an unknown (typically foreign) word.

In this way we have reduced 684k paraphrases in~the original Czech Meteor 
Paraphrase tables to~only 32k pairs of lemmas. We refer to~this table as
~\textit{filtered Meteor}.

\section{Simple substitution paraphrasing}
\label{lrec}
We experiment with several algorithms for paraphrasing reference sentences. 
This method is widely based on \newcite{kauchak}. Our algorithm differ extends it 
with several methods for selecting potential paraphrase pairs and in the choice 
of paraphrases.

\subsection{Candidate Selection}
We select potential paraphrases using two different methods. The first one is a 
simple greedy search similar to~\newcite{kauchak}, the other one uses automatic 
word alignment for selecting corresponding segments of~the reference sentence 
and the hypothesis.

\subsection*{Simple Greedy Method}
Let $ r_1,..., r_n $,$ w_1,...,w_m $ be the hypothesis and the reference 
sentence, respectively. We performed their tagging and extracted sets of lemmas 
$ W_{L} $, $ R_{L} $. Then, one-word paraphrase candidates are chosen as:
$$ C_{L} = \lbrace (r,w) | r \in R_{L} \smallsetminus W_{L} \wedge w \in W_{L}
\smallsetminus R_{L}  \rbrace $$

Multi-words candidates $ C_M $ are selected analogically from all sequences 
of words from the reference sentence and from the hypothesis. Maximum phrase 
length is seven words, because that is the length of~the longest paraphrases in 
the data. % Formally:
%
%\begin{gather*}
%C_{M} = \lbrace (<r_i,..,r_{i+x}>, <w_j,...,w_{j+y}>) | \: 1 \leq i \leq n-x \: \wedge \\ 
% \: 1~\leq~j \leq m-y  \: \wedge \: 0 \leq x,y \leq 6 \: \wedge \: (x \neq 0 \vee y \neq 0) \rbrace 
%\end{gather*}

\subsection*{Word and Phrase Alignments}
One possible way to make the algorithm more reliable is to restrict the 
application of paraphrases to words/phrases which are aligned to each other. We 
compute word alignment between the reference translation and MT system outputs 
using GIZA++ \cite{gizapp}.

However, if we used only our test data to create the alignment (13 x 3003 + 12 
x 3000 = 75039 sentence pairs), the alignment quality would be insufficient. In 
order to make the training data for word alignment larger, we take advantage of 
the fact that all outputs are translations of the same data and also add all 
pairs of system outputs to our data, creating over 1,000,000 \equo{artificial} 
sentence pairs. For example, the parallel data for WMT12 then looks as follows:

\begin{center}
\begin{tabular}{ll}
Source & Target \\
\hline
system 1 & system 2 \\
system 1 & system 3 \\
... & ...\\
system 1 & system 13 \\
system 1 & reference \\
system 2 & system 1 \\
system 2 & system 3 \\
... & ... \\
system 13 & reference \\
\end{tabular}
\end{center}

We also experiment with adding much larger synthetic parallel data created by
machine translation (note that we need Czech-Czech data) but there was no 
impact on the quality of paraphrasing so we follow the outlined approach which 
requires no additional data or processing.

The set of~one-word candidates $C_L$ is then simply the set of all word pairs 
such that there exists an alignment link between them. The set $C_M$ is 
extracted using phrase extraction for phrase-based MT, the standard consistency 
criterion is~applied \cite{Och99improvedalignment}.

\subsection{Paraphrasing}
We reduce the set $ C_{L} $ to pairs appearing in our paraphrase tables in the 
following way. If a word appears in several synonymous pairs we give preference 
to those found in~WordNet or even better in the intersection of paraphrases 
from WordNet and filtered Meteor. Similarly, we filter $ C_{M} $ to pairs also 
contained in the multi-word Meteor tables.

We evaluate three different paraphrasing methods which differ in the order of
substitution.

\begin{description}
\item[One-word only] We proceed word by word from the beginning of the reference 
sentence to~its end. If a~lemma of~a~word appears as~the first member of~a~pair 
in reduced $ C_{L} $, it is replaced by~the word from hypothesis that has its lemma
as the second element of~that pair, i.e., paraphrase from the hypothesis. Otherwise, 
we keep the original word from the reference sentence.
\item[One-word first] We use \textit{One-word only} and then we apply longer paraphrases.
In that case we move ahead from the longest paraphrases to the shortest. That is because 
Meteor contains often even components of~phrases and we could substitute, instead of~whole 
phrase, only part of~it. We do not attempt to replace any word that was already changed 
before.
\item[Multi-word first] We substitute the longest confirmed paraphrases from
$ C_{M} $ and move to the shorter ones. We replace again only sequences that have not
been substituted yet. After this, we paraphrase the remaining unchanged words
with the \textit{One-word only} method.
\end{description}

\subsection{Depfix}
As we substitute a word form directly from a hypothesis, it may happen that a 
resulting new reference is not grammatically correct. To rectify this, we apply
Depfix \cite{depfix} -- an automatic post-editing system which is able to fix 
Czech sentences containing grammatical errors.

Depfix was originally designed for post-editing outputs of English-to-Czech 
phrase-based machine translation. It consists of a set of~linguistically-motivated 
rules and a statistical component that correct various kinds of errors, especially 
in grammar (e.g. morphological agreement), using a range of natural language 
processing tools to provide analyses of the input sentences.

We observe that the errors that appear in the outputs of our paraphrasing 
algorithm are often similar to some errors appearing in outputs of phrase-based
machine translation systems, e.g errors in morphological agreement are very 
common. This makes Depfix a good fit for fixing the errors, since typical 
grammar correcting tools, such as a grammar-checker in a word processor, focus
on errors that are typical for humans, not for machines. 

\subsection{Results}
Results of our method are presented in Table \ref{corrs13} as the Pearson 
correlation between human judgment and BLEU computed on our new references. 
All evaluated approaches outperform the~baseline (i.e., using the original 
reference sentences), the simplest one \textit{One-word only} performs 
best (\Fref{example} shows an example of this method).

\begin{table*}[tb]
\begin{center}
\scalebox{0.95}{
\begin{tabular}{l|ccc|ccc}
\multicolumn{7}{c}{\textbf{WMT12}}\\
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Greedy selection} & \multicolumn{3}{c}{Word alignment} \\
& No Depfix & Full Depfix & Limited Depfix & No Depfix & Full Depfix & Limited Depfix\\
\hline
One-word only     & 0.802 & 0.827 & \textbf{0.832} & 0.792 & 0.813 & 0.810 \\
One-word first    & 0.785 & 0.822 & 0.816 & 0.767 & 0.792 & 0.798 \\
Multi-word first & 0.768 & 0.810 & 0.804 & 0.761 & 0.781 & 0.778 \\
\end{tabular}}

\vspace{10pt}

Baseline~correlation: \textbf{0.749}
\vspace{10pt}
%\caption{Correlation of the human judgment and BLEU computed with the data from WMT12}
%\label{corrs12}
%\end{center}
%\end{table*}
%
%\begin{table*}[tb]
%\begin{center}
\scalebox{0.95}{
\begin{tabular}{l|ccc|ccc}
\multicolumn{7}{c}{\textbf{WMT13}}\\
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Greedy selection} & \multicolumn{3}{c}{Word alignment} \\
& No Depfix & Full Depfix & Limited Depfix & No Depfix & Full Depfix & Limited Depfix\\
\hline
One-word only     & 0.861 & \textbf{0.887} & 0.883 & 0.856 & 0.877 & 0.872 \\
One-word first    & 0.851 & 0.880 & 0.875 & 0.833 & 0.871 & 0.863 \\
Multi-word first  & 0.838 & 0.870 & 0.864 & 0.833 & 0.868 & 0.861 \\
\end{tabular}}

\vspace{10pt}

Baseline~correlation: \textbf{0.829}
\caption{Correlation of the human judgment and BLEU using the simple substitution method.}
\label{corrs13}
\end{center}
\end{table*}
 
We use a freely available implementation\footurl{http://www.cnts.ua.ac.be/~vincent/scripts/rtest.py} of \newcite{meng:1992} to determine whether the difference in correlation
coefficients is statistically significant. The test shows that BLEU performs
better with our reference sentences with 99\% certainty. 

\begin{figure}[tb]
\begin{center}
\scalebox{0.89}{
\begin{tabular}{ll}
 Source &  \begin{tabular}{l}
  	\textit{The location alone is classic.} \\
	\end{tabular} \\
 \hline
 
 Hypothesis & \begin{tabular}{llll}
 			\textit{Samotné} & \textit{místo} & \textit{je} & \textit{klasické.} \\
 			Actual & place & is & classic \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	The place alone is classic. \\
	\end{tabular} \\

 \hline
 %\noindent\rule{8cm}{0.4pt}\\
 Reference & \begin{tabular}{llll}
 			\textit{Už} & \textit{poloha} & \textit{je} & \textit{klasická.} \\
 			Already & position & is & classic. \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	The position itself is classic. \\
	\end{tabular}  \\ 

 \hline
  New ref. & \begin{tabular}{llll}
 			\textit{Už} & \textit{místo} & \textit{je} & \textit{klasická.} \\
 			Already & place & is & classic \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	*The place itself is classic. \\
	\end{tabular} \\
 \hline 
  Depfixed ref. & \begin{tabular}{llll}
 			\textit{Už} & \textit{místo} & \textit{je} & \textit{klasické.} \\
 			Already & place & is & classic \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	The place itself is classic. \\
	\end{tabular} \\
 \hline  
 
\end{tabular}
}
\caption{Example of the \textit{One-word only} method. The~hypothesis is 
grammatically correct and has very similar meaning as the reference sentence. 
The new reference is closer in wording to the hypothesis, but there is no 
agreement between the noun and adjective. Depfix resolves the error and the 
final reference is correct and much similar to~the hypothesis.}
\label{example}
\end{center}
\end{figure}

Multi-word paraphrases are very noisy and while they do bring the system 
outputs closer to the reference (the average BLEU score of the systems 
increases), they often propose non-equivalent translations or violate the 
correctness of the sentence, thus blurring the differences between systems.

When paraphrasing is restricted by word alignment, all methods perform worse. 
As Table \ref{replaced} shows, the number of applied paraphrases is much lower: 
while the proportion of correct paraphrases is higher, their amount is reduced 
too much and overall, our technique is harmed by this restriction. 

\begin{table}[tb]
\begin{center}
\scalebox{0.85}{
\begin{tabular}{l|cc|cc}
\multicolumn{5}{c}{\textbf{WMT12}}\\
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Greedy selection} & \multicolumn{2}{c}{Word alignment} \\
& Words & Phrases & Words & Phrases \\
\hline
One-word only     & 1.59 & --   & 0.86 &  --  \\
One-word first    & 1.59 & 0.23 & 0.86 & 0.22 \\
Multi-word first  & 1.38 & 0.31 & 0.81 & 0.27 \\
\end{tabular}}
\vspace{10pt}

\scalebox{0.85}{
\begin{tabular}{l|cc|cc}
\multicolumn{5}{c}{\textbf{WMT13}}\\
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Greedy selection} & \multicolumn{2}{c}{Word alignment} \\
& Words & Phrases & Words & Phrases \\
\hline
One-word only    & 1.33 &  --  & 0.76 & --   \\
One-word first   & 1.33 & 0.20 & 0.76 & 0.20 \\
Multi-word first & 1.04 & 0.68 & 0.74 & 0.24 \\
\end{tabular}}
\caption{Average number of replaced words/phrases.}
\label{replaced}
\end{center}
\end{table}

On the other hand, applying Depfix is always beneficial, with the positive 
effects ranging from 0.017 up to 0.042. This supports our assumption of the 
importance of grammatical correctness of the created references. However, the 
\textit{limited} version is not optimally chosen and performs worse than the 
\textit{full} version in most cases.

Results on the data from WMT13 and WMT12 are very similar. Again, paraphrasing 
helps to increase the accuracy of the evaluation, even though the differences 
on the WMT13 data are not as~big due to much higher baseline. This is also 
reflected in~the smaller amount of substitutions.

\subsection{Meteor without paraphrase support}
\label{parmesan}
Based on the positive impact of filtering Meteor Paraphrase Tables for targeted
lexical paraphrasing of reference sentences, we experiment with the filtering 
them yet again, but this time as~an~inner part of~the Meteor evaluation metric 
(i.e. for the paraphrase match).

\begin{table*}[tb]
\begin{center}

\scalebox{0.99}{
\begin{tabular}{r|c|l}
setting & size & description of the paraphrase table \\
\hline
\textbf{Basic} & 684k & The original Meteor Paraphrase Tables \\
\textbf{One-word} & 181k & \textbf{Basic} without multi-word pairs\\
\textbf{Same POS} & 122k & \textbf{One-word} + only same part-of-speech pairs\\
\textbf{Diff. Lemma} & 71k & \textbf{Same POS} + only forms of different lemma\\
\textbf{Same Lemma} & 51k & \textbf{Same POS} + only forms of same lemma\\
\textbf{No paraphr.} & 0 & No paraphrase tables, i.e., exact match only\\
\textbf{WordNet} & 202k & Paraphrase tables generated from Czech WordNet\\
\end{tabular}
}
\caption{Different paraphrase tables for Meteor and their size (number of paraphrase pairs).}
\label{meteory}
\end{center}
\end{table*}

The filtering of~the paraphrase tables is performed analogically. We experiment
with seven different settings that are presented in~\Tref{meteory}. All of them
are created by reducing the original Meteor Paraphrase tables, except for the 
setting referred to~as~\textbf{WordNet}. In this setting, the paraphrase table 
is generated from one-word paraphrases in Czech WordNet to all their possible 
word forms appearing in~CzEng \cite{czeng}.
%
%Prior paraphrasing reference sentences and using Meteor with the \textbf{No 
%paraphr.} setting for computing scores constitutes Parmesan -- our submission
%to the WMT14 \cite{wmt14} for evaluation English-to-Czech translation. In the 
%tables with results, Parmesan scores are highlighted by the box and the best 
%scores are in bold. % \xxx{podle lopatkovy preformulovat}

\begin{table*}[tb]
\begin{center}
\scalebox{0.95}{
\begin{tabular}{l|ccccccc}
\multicolumn{8}{c}{\textbf{WMT12}}\\
\hline
reference & Basic & One-word & Same POS & Same Lemma & Diff. Lemma & No paraphr. & WordNet \\
\hline
Original & 0.833 & 0.836 & 0.840 & 0.838 & 0.863 & 0.861 & 0.863 \\
Before Depfix & 0.905 & 0.908 & 0.911 & 0.911 & 0.931 & 0.931 & 0.931 \\
New & 0.927 & 0.930 & 0.931 & 0.932 & 0.950 & \textbf{0.951} & \textbf{0.951} \\
%\hline
\end{tabular}} 

\vspace{10pt}

\scalebox{0.95}{
\begin{tabular}{l|ccccccc}
\multicolumn{8}{c}{\textbf{WMT13}}\\
\hline
references & Basic & One-word & Same POS & Same Lemma & Diff. Lemma & No paraphr. & WordNet \\
\hline
Original & 0.817 & 0.820 & 0.823 & 0.821 & 0.850 & 0.848 & 0.850 \\
Before Depfix  & 0.865 & 0.867 & 0.869 & 0.868 & 0.895 & 0.895 & 0.894 \\
New  & 0.891 & 0.892 & 0.893 & 0.892 & \textbf{0.915} & \textbf{0.915} & \textbf{0.915} \\
%\hline
\end{tabular}
}
\caption{Pearson's correlation of Meteor and the human judgment.}
\label{results:12:13}
\end{center}
\end{table*}

The results of our experiments are presented in \Tref{results:12:13}, they are 
very consistent for WMT12 and WMT13. We show that independently of~a~reference 
sentence used, reducing the Meteor paraphrase tables in evaluation is always 
beneficial.

Tests show that prior paraphrasing references and Meteor without paraphrase
support significantly outperform the original Meteor on our data.

\textbf{Diff. Lemma} and \textbf{WordNet} settings give the best results on the 
original reference sentences. That is because they are basically a limited 
version of the paraphrase tables we use for creating our new references, which 
contain both all different lemmas of the same part of speech from Meteor 
Paraphrase tables and all lemmas from the WordNet.

The main reason of the worse performance of the metric when employing the 
Meteor Paraphrase tables is the noise. The metric may award even parts of the 
hypothesis left untranslated, as the original Meteor Paraphrase tables contain 
some English words and their Czech translations as paraphrases. There are for
~example pairs: \textit{pšenice} - \textit{wheat}\footnote{In all examples the 
Czech word is the correct translation of the English side.}, \textit{vůdce} - 
\textit{leader}, \textit{vařit} -	\textit{cook}, \textit{poloostrov} - 
\textit{peninsula}. For these reasons, the differences among the systems are 
more blurred and the metric performs worse than without using the paraphrases. 

\section{Paraphrasing using Machine Translation}
\label{MT}
While the previous method offers good result, it is very limited. We would like
to be able to use longer paraphrases, word order changes, switch between active 
and pasive construction, etc.

For this purpose, we employ machine translation itself -- there are many tools 
for MT and there is close resemblance between translation and paraphrasing. 
They both attempt to preserve the meaning of a sentence, the first one between 
two languages and the second one within one language by different word choice. 
It seems only natural to attempt to adjust some MT tools to translate within a 
single language for targeted paraphrasing. 

We describe this attempt on two types of MT systems -- phrase-based and 
rule-based. Initially, we experiment with the freely available SMT system 
Moses. \cite{moses} We create translation from Czech WordNet and the Meteor 
Paraphrase tables. We extended Moses by a new feature that makes the 
translation targeted. 

However, the results of this method are inconclusive. In the view of errors 
appearing in the new paraphrased sentences, we propose another solution -- 
targeted paraphrasing using a rule-based translation system included in the NLP 
framework Treex. \cite{treex}

\subsection{Paraphrasing using Moses}
Moses is a freely available statistical machine translation engine. In a 
nutshell, statistical machine translation involves the following phases: 
creating language and translation models, parameter tuning and decoding. We use 
Moses in the phrase-based setting.

A language model is responsible for a correct word order and grammatical 
correctness of the translated sentence. A translation model (phrase table) 
supplies all possible translations of a word or a phrase. Models are assigned 
weights which are learned during the parameter tuning phase.

During the decoding phase, all these models are combined to maximize 
$ \sum_i \lambda_i \phi_i (\bar{f},\bar{e}) $, where  $ \lambda_i $ is a weight 
of a the sub-model $ \phi_i $ and $ \bar{f},\bar{e} $ is a hypothesis and 
source sentence, respectively. In our case, we want to make a reference 
sentence closer to a corresponding machine translation output -- $ \bar{e} $ is 
the reference sentence and $ \bar{f} $ is a new synthetic reference.

On its own, this setting could create paraphrases, but they would be just 
random paraphrases of the reference sentence -- their similarity to our 
original hypotheses would not be guaranteed. Therefore, we also add a new 
feature for targeted paraphrasing to Moses.

\subsubsection{Language model}
We create the language model (LM) using the SRILM toolkit \cite{srilm} on the 
data from the Czech part of the Czech-English parallel corpus CzEng. 

\subsubsection{Translation models}
Each entry in Moses phrase tables contains a phrase, its translation, several
feature scores (translation probability, lexical weight etc.), and optionally
also alignment within the phrase and frequencies of phrases in the training 
data. The phrase tables are learned automatically from large parallel data.
As we do not have any large corpora of Czech-Czech parallel data, we create the 
following two ``fake'' translation models for paraphrasing from our paraphrase 
tables. 

\subsubsection*{Enhanced Meteor tables}
This table was created from the Czech Paraphrase Meteor tables. Each paraphrase 
pair comes with a pivoting score which we adapt as a feature in out phrase 
table. Further, we add our own paraphrase scores, acquired by 
\textit{distributional semantics}. Distributional semantics assumes that two 
phrases are semantically similar if their contextual representations are 
similar. \cite{miller-91}

We collect all contexts (words in a window of limited size) in which Meteor 
paraphrases occur in the Czech National Corpus \cite{SYN2010} and then measure 
context similarity cosine distance, taking into account the number of word 
occurrences) for each pair of paraphrases. 

We add six scores for each pair of paraphrases according to the size of the 
context window used (1-3 words) and whether word order played a role in the 
context. 

\subsubsection*{One-word paraphrase table}
We first create a set of all words from Czech side of CzEng appearing at 
least five times to exclude rare words and possible typos. We also add all 
words appearing in the MT outputs and the reference sentences. Morphological 
analysis of the words was then performed using Morče. 

For every word $ x $ from this set, we add to this translation table every pair 
of words that fulfils at least on of the following requirements:

\begin{itemize}
\item $ x,x $ (not every word should be paraphrased)
\item $ x,y $, if lemma of $ x $ is lemma of $ y $ (some word 
might have different morphology in the paraphrased sentence)
\item $ x,y $, if lemma of $ x $ and lemma of $ y $ are paraphrases according 
to Czech WordNet PDT 1.9.
\item $ x,y $, if lemma of $ x $ and lemma of $ y $ are paraphrases according 
to the filtered Meteor.
\end{itemize}

These categories constitute the first four scores in the phrase table. A pair 
of words gets score $ e $ if they fall in a given category, 1 ($e^0$) 
otherwise.\footnote{Phrase-table scores are considered log-probabilities.} 
This phrase table contains more than 1,100k pairs of words.

We add another score expressing POS tag similarity between the two words. It is 
computed $ e^{\frac{1}{a+1}}$, where $ a $ is the minimal Hamming distance 
between tags of the words. This probability should reflect how morphologically 
distant the paraphrases are. 

\subsubsection{Feature for targeted paraphrasing}
In order to steer the MT decoder (translation engine) in the direction of the 
hypotheses, we implemented to Moses an additional feature, which measures the 
overlap with the hypothesis. In order to keep its computation tractable during 
search, the overlap is defined simply as the number of words from the 
hypothesis confirmed by the reference translation.

Integration into the beam search algorithm used in phrase-based decoding
requires us to keep track of feature state (i.e. reference words covered) to
allow for correct hypothesis recombination. We also implemented an estimator of
future phrase score, defined as the number of reference translation words
covered by the given phrase. Our code is included in
Moses.\footurl{https://github.com/moses-smt/mosesdecoder/}

\subsubsection{Parameter tuning}

\begin{table*}[tb]
%\begin{center}
\begin{tabular}{r|l|c|c}
setting & reference sentence used & correlation & avg. BLEU \\
\hline
\textbf{Baseline} & original reference sentence, no paraphrasing & \textbf{0.75} & 12.8 \\
\textbf{Paraphrased} & paraphrased by Moses using MERT-learned weights  & 0.50  & 15.8 \\
\textbf{LM+0.2}  & paraphrased by Moses with LM weight increased by 0.2  & 0.24 & 9.1 \\
\textbf{LM+0.4} & paraphrased by Moses with LM weight increased by 0.4  & 0.22 & 6.7 \\
\end{tabular}
\caption{Description of basic settings and the results - Pearson's correlation of BLEU and the
human judgment, the average BLEU scores.} 
%korelace metrik - 0.981
\label{settings}
%\end{center}
\end{table*}


\begin{figure*}[tb]
\begin{center}
\begin{tabular}{l|r}
 \textbf{Source } &  \textit{Paclík claims he would dare to manage the association.} \\
 \hline
 
 \textbf{Baseline} & Paclík tvrdí , že by si na vedení asociace troufl.\\
           & \textit{Paclík claims he would dare to lead the association.} \\

 \hline
 \textbf{Hypothesis} & Paclík tvrdí, že by se odvážil k řízení komory. \\
            & \textit{Paclík claims he would find the courage to control the chamber.}  \\ 

 \hline
 \textbf{Paraphrased} & Paclík tvrdí, že by se na řízení organizace troufl. \\
             & \textit{*Paclík claims he would dare to control the organization.}\\
 \hline 
 \textbf{LM+0.2} & Paclík tvrdí, že by si troufl na řízení ekonomiky. \\
               &\textit{ Paclík claims he would dare to control the economy.} \\
  
\hline 
 \textbf{LM+0.4} & Říká se, že Paclík si troufl na řídící rady. \\
               & \textit{They say that Paclík ventured to governing boards.} \\
%Lexical & Paclík tvrdí , že by se na řízení asociace troufl .\\
%Lexical boosted by 20 & Paclík tvrdí , že by si troufl na řízení ekonomiky .\\
%asociace, komora nikde nejsou jako parafraze

\end{tabular}
\caption{Example of the targeted paraphrasing. The~hypothesis is grammatically 
correct and has very similar meaning as the source sentence. The new reference 
is closer in wording to the hypothesis, but there is an error in a word choice. 
The sentences created with increased weights of the language model are both 
grammatically correct, but the sentence lost its original meaning.}
\label{example:Moses}
\end{center}
\end{figure*}

We use the minimum error rate training (MERT) \cite{mert} to find the optimal 
weights for our models. MERT asserts the weights to maximize the translation 
quality, which is measured with BLEU. We employ the reference sentences and the 
highest rated MT output as the parallel data for tuning. 

This method, however, turned out not to be optimal for our setting. Our feature 
for targeted paraphrasing naturally obtains the highest weight as it provides 
an oracle guide towards the hypothesis.

Other important models, e.g. the language model, get comparably very small 
weights. The paraphrased sentences tend to be closer to the hypothesis, but not 
grammatically correct. Therefore, we experiment with increasing the weight of 
the language model manually. 

\subsubsection{Results}
We compare four different basic settings, the results are presented in 
\Tref{settings} as the Pearson’s correlation coefficient of BLEU and the human 
judgment. In contrast to  our previous results, the baseline score is not 
exceeded by any of our paraphrasing methods.
%A visualization of the results is shown in \Fref{visualization}. 

There are several reasons for the clear decrease in correlation with 
paraphrased references. Hypotheses generated by the \textbf{Paraphrased} 
setting, while obtaining a significantly higher BLEU score, were mostly 
ungrammatical and reduced the correlation of our metric.

The small weight of the language model seems to be the problem, but its 
increase brings even more chaos. It creates hypotheses which are nice and 
grammatically correct but often wholly unrelated to the source sentence.

This shows that our paraphrase table noise filtering was by no means sufficient 
and there is a lot of noise in our phrase tables. Furthermore, the MT output 
might be far from being a correct sentence -- given the high weight for the 
targeted paraphrase feature, we essentially transform the correct reference 
sentences to incorrect hypotheses at all cost, using our noisy phrase tables.

Our targeting feature is also not ideal -- it ignores word order and operates
only on the word level (it does not model phrases). Ungrammatical translations
with scrambled word order are considered perfectly fine so long as the
translation contains the same words as the reference. So while the feature does
provide a kind of oracle, it does not guarantee reaching the best possible
translation in terms of BLEU score, let alone a grammatical translation.

Another problem is illustrated by very small weights assigned to our 
translation models. In fact, the highest weight was assigned to the tag 
similarity feature. This shows that our model features (Meteor score and 
distributional similarity scores) fail to distinguish good paraphrases from the 
noise. 

The combination of noise in the translation tables and the boosted language
model then caused that during the decoding phase, the most common paraphrase
according to the language model with a similar tag got the preference. 

\Fref{example:Moses} represents an example of our paraphrasing method. The~hypothesis 
is grammatically correct and has a very similar meaning as the reference 
sentence. The new paraphrased reference is slightly closer in wording to the 
hypothesis, but there is an error due to a bad word choice. The boosted 
language model reduces errors, however the meaning of the sentences is shifted. 
In the \textbf{LM+0.4} setting, they also differ a lot in wording from both the 
hypothesis and the reference sentence.

\subsection{Rule based Machine Translation}
Based on previous results, Moses does not seem to be the optimal tool for our 
task, especially unless we have at our disposal better paraphrasing tables, or
we develop better targeting function. %TODO: lepsi tuning? 

Furthermore, there may be a better solution than a phrase-based translation
system -- namely a highly modular NLP software system Treex, developed for 
TectoMT, a rule-based machine translation system that operates on a deep 
syntactic layer.

Treex implements the stratificational approach to language, adopted from the 
Functional Generative Description theory \cite{FGP} and its later extension by 
the Prague Dependency Treebank \cite{PDT3.0}. It represents sentences in four 
layers: word layer (w-layer), morphological layer (m-layer), 
shallow-syntax/analytical layer (a-layer) and deep-syntax/tectogrammatical 
layer (t-layer).

In TectoMT, a sentence in a source language is analyses from w-layer to 
t-layer, transferred to a second language t-layer, and a translation is 
generated from t-layer to a-layer.

%Treex is opensource and is available on github (link), including the two
%blocks that we contributed (link, link), as well as the scripts to run our method
%(link). The following text describes the implementation of our approach, and
%corresponds to a Treex scenario (link).

Using Treex, we can easily overcome some of the problems of paraphrasing using 
Moses. First of all, we can only compare two sentences and there is no need to 
create paraphrase tables, thus less space for the noise to interfere. Also 
there already is highly developed machinery to avoid ungrammatical sentences. 
We can change only parts of sentences that are dependent on the changed word, 
thus keeping the rest of the sentence correct and creating more conservative 
references.
%
%We performed simple paraphrasing using Treex and started with implement the
%reordering module.
\subsection{Paraphrasing using Treex}
The analysis and generation pipeline is taken from the TectoTM system. In our 
setting, we transfer both hypothesis and reference sentence to the t-layer, 
where we replace the transfer phase with module for t-lemma paraphrasing. 
After paraphrasing we perform synthesis to a-layer, where we plug a reordering 
module a continue with synthesis to the w-layer. 

\subsubsection{Analysis}
The analysis from w-layer to a-layer includes tokenization, POS-tagging and 
lemmatization using MorphoDiTa \cite{morphodita}, dependency parsing using the 
MSTParser \cite{McDonald:2005} adapted by \newcite{Novak:2007}, trained on PDT.

Surface-syntax a-tree is then converted into deep-syntax t-tree. Auxiliary 
words are removed, with their function now represented using t-node attributes 
(grammatemes and formemes) of autosemantic words that they belong to (e.g. two
a-nodes of the verb form “has been played” would be collapsed into one t-node
“play”, with the tense grammateme set to past and the diathesis grammateme
set to passive; “in May” would be collapsed into “May” with formeme in+X).

Morphological features are converted into grammatemes as well. The t-nodes
thus consist of a t-lemma and a set of attributes (a formeme and a set of 
grammatemes); however, the surface form of the words is not represented on 
t-layer.

\subsubsection{Paraphrasing}
The paraphrasing block T2T::ParaphraseSimple is freely available at GitHub (TODO: url). 
If no translation t-node bears the t-lemma A of the reference t-node R, and
if there exists a t-lemma B that is a paraphrase of the t-lemma A according
to WordNet or filtered Meteor, and there is a translation node T which bears 
the t-lemma B, and no t-node of the reference bears the t-lemma B, then the 
t-lemma of the reference t-node R is changed from A to B.

The other attributes of the t-node are kept unchanged based on the theory that
semantic properties are independent of the t-lemma. However, in practice, there 
is at least one case where this is not true: t-nodes corresponding to nouns are 
marked for grammatical gender, which is very often a grammatical property of 
the given lemma with no effect on the meaning (for example, “a house” can be 
translated either as a masculine noun “d\o{u}m” or as feminine noun “budova”),
%; we really believe this should be marked on the lemma only, but this is not the case in Treex. 
Therefore, when paraphrasing a t-node that corresponds to a noun, we delete 
the value of the gender grammateme, and let the subsequent synthesis pipeline 
generate the correct value of the morphological gender feature value (which is 
necessary to ensure correct morphological agreement with surrounding words, 
such as adjectives and verbs).

\subsubsection{Synthesis from t-layer to a-layer}
In this phase, a-nodes corresponding to auxiliary words and punctuation are 
generated, morphological feature values on a-nodes are initialized and set to 
enforce morphological agreement among the nodes. Correct word forms based 
on lemma and POS, and morphological features are generated using MorphoDiTa.

\subsubsection{Tree-based reordering}
The reordering block A2A::ReorderByLemmas is freely available at GitHub (TODO: url).

The idea behind the block is to make the word order of the new reference as 
similar to the word order of the translation, but with some tree-based 
constraints to avoid ungrammatical sentences. The general approach is to 
reorder the subtrees rooted at modifier nodes of a given head node so that they 
follow in an order that is on average similar to their order in the translation.

%As the ultimate MT quality metric we apply to compare the translation and
%the reference is word based, we align translation and reference words that have
%an identical lemma. Reordering reference words with a lemma that does not
%appear in the translation has little effect on the resulting score given by the
%metric, we thus treat those as unaligned words. For simplicity, we also treat
%repeated words as unaligned, as it is not straightforward to decide which of the
%possible alignments to use; however, repeated words are rather rare. 
%TODO future work – align more words.
%Our reordering proceeds in several steps.

Each a-node has order, i.e. position in the sentence. % (the a-nodes are number
%by numbers from 1 to N where N is the number of tokens in the sentence).
First, we define the \textit{MT order} of each reference a-node as the order of its
corresponding translation a-node, i.e. a node with the same lemma. We define
the MT order only if there is exactly one a-node with the given lemma in the
translation and exactly one in the reference; thus, the MT order is undefined
for some nodes.

Next, we compute the subtree MT order of each reference a-node R as the
average MT order of all a-nodes in the subtree rooted at the a-node R (including
the MT order of R itself). Only nodes with a defined MT order are taken into
account; thus, also the subtree MT order is undefined for some nodes. The
calculation of subtree MT order is performed recursively and is thus efficient.
Then, we iterate over all a-nodes, and for each a-node with at least one
dependent node, we reorder the sequence S of the head a-node H and its de-
pendent a-nodes D i . For each of the nodes (H, D i ), we define its sorting order.
The sorting order of H is defined as its MT order The sorting order of each
dependent node D i is defined as its subtree MT order.
If for a node its sorting order is undefined, it is set to the sorting order of
the node that precedes it in the sequence S, thus favouring neighbouring nodes
(or subtrees) to be reordered together in case there is no evidence that they
should be brought apart from each other. If the node is first in the sequence S,
its sorting order is defined as 0.
Additionally, a 1/1000th of the original order of the node is added to its
sorting order (for each D i as well as for H); this is to break ties in such a way
that the original ordering of the nodes is preferred to reordering them.
And finally, the sequence of the nodes is reordered according to their sorting
order. If a node is a root of a subtree, the whole subtree is moved and its
internal ordering is kept; unless it is the H node, which, obviously, is always
moved alone, without its dependents.
We do not handle non-projective edges in any special way, so they always
get projectivized if they take part in a reordering process, or kept if they do
not. However, no new non-projective edges are created in the process – this is
ensured by always moving the subtrees at once.
Please note that each node can take part in at most two reorderings – once
as the H node and once as a D i node. Moreover, the nodes can be processed in
any order, as a reordering does not influence any other reordering.


%Example:
%Ref: Jenže teď už čekat nehodlá a do dalšího života stanovila jasná pravidla.
%MT: Nicméně nechce čekat a nastavit jasná pravidla pro svůj budoucí život.
%NO_reordering: Jenže teď už nechce čekat a do dalšího života stanovila jasná pravidla.
%RES:Jenže teď už nechce čekat a nastavila jasná pravidla do dalšího života.

\begin{figure*}[tb]
\begin{center}
\scalebox{0.95}{
\begin{tabular}{l|r}
 \textbf{Source}  &  \textit{However, she does not want to wait any longer, and set clear rules for her future life.} \\
 \hline
 
 \textbf{Reference} & Jenže teď už čekat nehodlá a do dalšího života stanovila jasná pravidla.\\
           & \textit{But now, she doesn't intend wait any further and she established for her future life clear rules.} \\

 \hline
 \textbf{Hypothesis} &  Nicméně nechce čekat a nastavit jasná pravidla pro svůj budoucí život. \\
            & \textit{However, she doesn't want to wait and set up clear rules for her future life.}  \\ 

 \hline
 \textbf{No reordering} & Jenže teď už nechce čekat a do dalšího života stanovila jasná pravidla. \\
             & \textit{But now, she doesn't want wait any further and she set up for her future life clear rules.}\\
 \hline 
 \textbf{New reference} & Jenže teď už nechce čekat a nastavila jasná pravidla do dalšího života. \\
               &\textit{But now, she doesn't want wait any further and she set up clear rules for her future life. } \\

\end{tabular}
}
\caption{Example of the Treex paraphrasing method.}
\label{example:Treex}
\end{center}
\end{figure*}

\subsubsection{Results}
\begin{table*}[tb]
\begin{center}
\begin{tabular}{l|ccc}
\multicolumn{4}{c}{\textbf{WMT12}}\\
\hline
reference & original & before reordering & new reference \\
\hline
BLEU & 0.751 & 0.783  & 0.804 \\
Ex.Met & 0.848 & 0.900 & 0.903 \\
%\hline
\end{tabular} 

\quad

\begin{tabular}{l|ccc}
\multicolumn{4}{c}{\textbf{WMT13}}\\
\hline
reference & original & before reordering & new reference \\
\hline
BLEU & 0.834 &  0.873 & 0.878 \\
Ex.Met & 0.848 & 0.893 &  0.893\\
%\hline
\end{tabular}

\caption{Pearson's correlation of Meteor and the human judgment.}
\label{treex_results:12:13}
\end{center}
\end{table*}

Example of the method, see \Fref{example:Treex}, TODO: nejake ilustracni stromy, result see \Tref{treex_results:12:13}

\section{Future Work}
We would like to work to enhance Treex paraphrases, next step could be multiword 
paraphrases, better word reordering, deleting unnecessary wordy, syntactic paraphrases.
 pekny by byly
taky nejaky ty hierarchicke pravidla. 

Ze zvedavosti bych to taky chtela aplikovat na dalsi jazyky pro ktere ma treex
analyzu a syntezu, AJ, ES...

\bibliographystyle{acl}
\bibliography{biblio}

\newpage
\newpage 
\section*{Appendix}
\subsection*{Automatic Filtering of Meteor tables}
\label{multiword_filtering}
Filtering strategies described in~this section are based on assigning a score 
to each paraphrase pair. We then gradually remove paraphrases with low scores 
and measure the effect on~the final correlation of~our metric.

The first, straightforward approach is to use the paraphrase scores already
provided in Meteor. They are based on~phrasal translation probabilities and it 
corresponds to~paraphrase probability in~the pivoting model.

We propose an alternative scoring based on pivoting and lexical translation
scores:

$$\text{lex\_p}(\mathbf{s},\mathbf{t}) = \sum_{s \in \mathbf{s}}\sum_{t \in
\mathbf{t}}\sum_{pivot}\text{lex}(s|pivot)\text{lex}(pivot|t)$$

In this case, pivots are all words aligned to both $s$ and $t$ in the parallel
data. To get lexical translation probabilities, we use maximum likelihood
estimation from single best word alignment computed on CzEng 1.0
\cite{czeng10:lrec2012}. We refer to this score as \emph{lexical pivoting}.

We use random selection as the baseline -- paraphrases are simply shuffled and 
we then use the first 10, 20,$\ldots$ percent of them.

We only evaluate the filtering techniques on the WMT12 data. First, we attempt
to filter one-word paraphrases and use the cleaner paraphrase table in the
\emph{one-word-only} paraphrasing strategy. Note that our paraphrase table has
already been filtered using the error-analysis based filtering described in 
\Sref{Data}.

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.55]{filtering-lexical-cropped.pdf}
\caption{Comparison of automatic filtering techniques for~\emph{one-word} 
paraphrases on WMT12 data.} 
\label{fig:filtering-lexical}
\end{center}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.55]{filtering-mwe-cropped.pdf}
\caption{Automatic filtering of multi-word paraphrase for~the 
\textit{multi-word-first} scenario on WMT12 data.}
\label{fig:filtering-mwe}
\end{center}
\end{figure}


\Fref{fig:filtering-lexical} shows the performance of different filtering
techniques for one-word paraphrases. Relying on Meteor scores proves worse than
random selection. Using lexical pivoting, we can keep a high correlation even 
if we throw away as much as 90\% of the paraphrases, however we do not improve 
(by~a~relevant margin) upon the baseline correlation of 0.802 achieved by
\emph{one-word-only} paraphrasing with the full paraphrase table.

We evaluate the best-performing technique also in the \textit{multi-word-first}
scenario where we use it for filtering multi-word paraphrases (see
\Fref{fig:filtering-mwe}). As we reduce the number of paraphrases, we observe a
considerable improvement of~correlation, however we never outperform
\textit{one-word-only} or \textit{one-word-first}. In this case, the filtering
simply mitigates the damage done by the multi-word paraphrases. We
cannot hope to achieve a higher score without a~more fine-grained grip on what
a~good multi-word paraphrase is.

\end{document}
