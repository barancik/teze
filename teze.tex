\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[utf8]{inputenc} %http://depling.org/depling2015/formats/depling2015latex/depling2015latex.zip
\usepackage{color}
\usepackage{xspace}
\usepackage{url}

\usepackage{amsmath}
%%\usepackage{color}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}


\def \xxx#1{\textbf{\textcolor{red}{xxx: #1}}}
\def \todo#1{\textbf{\textcolor{red}{todo: #1}}}
\def\equo#1{``#1''}

\def\Tref#1{Table~\ref{#1}}
\def\Fref#1{Figure~\ref{#1}}
\def\Sref#1{Section~\ref{#1}}
\newcommand{\out}[1]{\textcolor[rgb]{0.8,0.8,0.8}{\textbf{#1}}}
\newcommand{\ml}[1]{\textcolor{red}{\raisebox{.2ex}{\tiny ML:~}#1}}
\def\footurl#1{\footnote{\url{#1}}}

%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\title{Parmesan: Meteor without Paraphrases with Paraphrased References}

\author{Petra Barančíková \\
  Institute of Formal and Applied Linguistics \\
  Charles University in Prague, Faculty of Mathematics and Physics\\
  Malostranské náměstí 25, Prague, Czech Republic \\
  {\tt barancikova@ufal.mff.cuni.cz} \\}

\date{}
%
%This paper describes Parmesan, our submission to the 2014 Workshop on Statistical Machine Translation (WMT) metrics task for evaluation English-to-Czech translation. 
%We show that the Czech Meteor Paraphrase tables are so noisy that they actually can harm the performance of the metric. However, they can be very useful after extensive filtering in targeted paraphrasing of Czech reference sentences prior to the evaluation.
%Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences. It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data. However, this result was not confirmed by this year's data.

\begin{document}

\maketitle
\begin{abstract}
This thesis is concentrated on improving an accuracy of machine translation to 
Czech language using targeted paraphrasing. We develop system that creates new
synthetic reference that are closer in wording to a corresponding machine 
translations. 

Using these new reference sentences makes evaluation using traditional metrics 
is more reliable as they lack of paraphrase support and have no sympathy for a 
free word order.

We believe that after implementation, the system for generating new synthetic
references will be very beneficial not only for MT evaluation, but mainly
inside MT systems for improved tuning and development of the systems. This way,
it will directly influence the quality of machine translation itself.

%Trade-off between simplicity and quality of MT metric!!
\end{abstract}

\section{Introduction}
Since the very first appearance of machine translation (MT) systems, a 
necessity for their objective evaluation and comparison arose. The traditional
human evaluation while being the most reliable has serious drawbacks -- it is 
time-consuming and expensive.
%, although there were recently successful attempts to overcome this problem 
%using crowdsourcing techniques. [\markcite{{\it Callison-Burch}, 2009]} and 
%[\markcite{{\it Bentivogli et al.}, 2011]} demonstrate that the average score of 
%high number of non-expert low-paid annotators shows high agreement with gold 
%standards provided by expert annotators in various natural language processing 
%(NLP) tasks including machine translation evaluation.

However, the main problem of human evaluation is that it is highly dependent on 
the person annotating and there is generally very low agreement between several 
human annotators. %footnote - example, only blah blah correlation during \cite{wmt12}.
Moreover, it is practically impossible to repeat the evaluation with the same
results (TODO: nejaka citace??).
%[Bojar - kniha] %nizka shoda pro cestinu?!! Ani ne kniha, ale spis wmt13, co 14?, 
% kde to byla nejak hrozive nizka shoda..

Due to being slow and unreproducible, it is impossible to use human evaluation
for tuning and development of MT systems. Well-performing automatic MT 
evaluation metrics are essential precisely for these tasks.

The first metric correlating well with human judgment was BLEU \cite{bleu}.
Due to its simplicity and language-independence, it still remains the most 
common metric for MT evaluation, even though other, better-performing metrics 
exist. \cite{wmt14} 

#Evidence that BLEU correlations are not as high as previously
thought (Callison-Burch 2006, Koehn and Monz, 2006). This is particular true at a sentence level (Blatz et al. 2003).

%BLEU (and other commonly used metrics - NIST \cite{nist}, TER \cite{ter} etc.) 
%disregards synonymous phrases and word form variants. One way how to  alleviate this 
%drawback is to include paraphrases to the evaluation metric (e.g. METEOR \cite{meteor}). 

%Note that the task of an MT metric is essentially one of identifying  whether the translation produced by a system is a paraphrase of the reference translation.

BLEU is computed from the number of phrase overlaps between the translated
% sem by se hodil asi opravdovej vzorecek misto jen zminy. nebo jen odkaz
% do sekce Automatic metrics?? asi tak
sentence (hypothesis) and the corresponding reference sentences, i.e., 
translations made by a human translator. Its main advantage is its simplicity 
and language independence. However, the standard practice is using only one 
reference sentence and BLEU then tends to perform badly. As there are many 
translations of a single sentence, even a perfectly correct machine translation 
might get a low score due different wording and disregarding synonymous 
expressions (see \Fref{example_of_BLEU_malfunction}). This is especially valid 
for morphologically rich languages with free word order like the Czech 
language. \cite{bojar-tackling-sparse-data}

\begin{figure*}[htb]
\begin{center}
\begin{tabular}{ll}
 Original sentence &  \begin{tabular}{l}
  	\textit{Banks are testing payment by mobile telephone} \\
	\end{tabular}  \\
 \hline
 
 MT output & \begin{tabular}{llllll}
 			\textit{Banky} & \textit{zkou\v sej\'i} & \textit{platbu} & \textit{pomoc\'i} & \textit{mobiln\'iho} & \textit{telefonu} \\
 			Banks & are testing & payment & with help & mobile & phone \\
			\end{tabular} \\
 & \begin{tabular}{l}
  	Banks are testing payment by mobile phone \\
	\end{tabular} \\

% MT output & \textit{Banky zkou\v sej\'i platbu pomoc\'i mobiln\'iho telefonu} \\
% 					 & Banks testing payment help mobile phone \\
% 					 & Banks are testing payment by mobile telephone \\
 \hline
 Reference sentence & \begin{tabular}{llll}
 			\textit{Banky} & \textit{testuj\'i} & \textit{placen\'i} & \textit{mobilem} \\
 			Banks & are testing & paying & by mobile phone \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	Banks are testing paying by mobile phone \\
	\end{tabular}
 
\end{tabular}
\caption{Example from WMT12 - Even though the translation is grammatically 
correct and the meaning of both sentences is very similar, it doesn't contribute 
to the BLEU score. There is only one unigram overlapping.}
\end{center}
\label{example_of_BLEU_malfunction}
\end{figure*}

TODO: Note that the task of an MT metric is essentially one of identifying  whether the translation produced by a system is a paraphrase of the reference translation.!!!

Our goal is  to achieve higher accuracy of MT evaluation by targeted 
paraphrasing of reference sentences, i.e. creating a new synthetic reference 
sentence that is still correct and keeps the meaning of the original sentence, 
but at the same time it is closer in wording to the MT output. 

The structure of the paper is as follows: in the next section, we present other
work in targeted paraphrasing for MT evaluation. In \Sref{Data}, we introduced
the data we use for our experiments.  In sections \ref{lrec} and \ref{MT}, 
we show paraphrasing based on phrase substitutions and machine translation 
itself. Finally, we conclude with avenues for further work.


\section{Related Work}
Using paraphrases is common for rozsirovani slovni zasoby , translating OOV words (}, improved tuning (Madnani, ...), ...

In machine translation evaluation

There are two attitudes towards solving the problem - one is to change the 
automatic metric itself to be tolerant to other sentence representation and the
other one is to pre-process automatically the reference sentence to new 
synthetic reference that is closer in wording to the original reference 
sentence and keeps its meaning and grammatical correctness.

There are several MT evaluation metrics that include paraphrase support --
TERplus \cite{terp}, ParaEval \cite{paraeval}. All of them depend on
their on source of paraphrases and only one of them -- METEOR 
\cite{meteor-wmt:2014} is available for the Czech language.
%WMT14 - Verta: WordNet, Beer: Meteor
%mozna je zminit i trochu vic???

There are several metrics that include paraphrase support, however only the 
Meteor metric is available for Czech language too.%TODO: preformulovat

%But this often awards even sentences with paraphrases that are not 
%grammatically correct. We take a different approach by transforming a reference 
%translation into a sentence that is closer to the MT output and keeps its 
%original meaning and correctness.

% madnani 2007 - Using Paraphrases for Parameter Tuning in Statistical Machine Translation 
% podivat se radsi jeste na vsechny ty Madnani
%In \cite{madnani:2010}, targeted paraphrasing via SMT is used to improve SMT itself 
%during the parameter optimization phase of machine translation. Correct hypotheses 
%are no longer needlessly penalized due to not having similar wording to a 
%corresponding reference sentence. %\xxx{Wubben}

\subsection{New synthetic references}
% Mozna tu zminit, ze tech metod parafrazovani je hromada (viz Lin a Pantel)
% krome pri ev

Targeted paraphrasing for MT evaluation is introduced in~\newcite{kauchak}. They 
focus on lexical substitution in Chinese-to-English translations.  They select 
all pairs of words for which one word appears in a reference sentence, second 
word in a hypothesis (the MT output), but none of them in both. They keep only 
pairs of synonymous words, i.e. words appearing in the same WordNet 
\cite{wordnet} synset. Each such a pair of words was further contextually 
evaluated. For every confirmed synonym, a new reference sentence is created by 
placing it to the reference sentence on the position of its synonym.

%Having a hypothesis $ H = h_1,...,h_n $ and its corresponding reference translation 
%$ R =r_1, ...,r_m $, they select a~set of~candidates 
%$ C = \lbrace \langle r_i,h_j \rangle  \vert r_i \in R \setminus H, h_j \in H \setminus R \rbrace $. 
% 
%$ C $~is reduced to pairs of words appearing in~the~same WordNet synset only. For every pair 
%$  \langle r_i,h_j \rangle \in C $, $ h_j $ is evaluated in the context $ r_1,...,r_{i-1},\square,r_{i+1},...,r_m $
%and if confirmed, the new reference sentence $ r_1,...,r_{i-1},h_j,r_{i+1},...,r_m $ is created.
%This way, several reference sentences might be created, all with a single changed word with respect
%to the original one.

This solution is not quite possible to apply to the Czech language. As Czech 
belongs among~inflective languages with rich morphology, a Czech word has 
typically many forms and the correct form depends heavily on its context, 
e.g., cases of nouns depend on verb valency frames. Changing a single word may 
result into not grammatical sentence. Therefore, we do not attempt to~change 
a~single word in~a~reference sentence but we focus on~creating one single 
correct reference sentence.

%mimo evaluaci se take cilene parafrazovani pouziva pri tuneni
%Madnani, Resnik: Improving Translation via Targeted Paraphrasing

TODO:
\cite{Zhou:2006}
ParaEval - an automatic evaluation framework, 
- it focuses on two prominent problems associated with Bleu =lack of support for paraphrase
matching and the absence of recall
= large  collection of paraphrases acquired through an unsupervised process -- identfying
phrase sets, that have the same translation in another language -- using state-of-the-art
statistical MT word alignent and phrase extraction method
= more stable and reliable comparison mechanism than bleu in both fluency and adequacy

\subsection{Automatic Metrics}

\subsubsection{Meteor}
The Meteor metric outperforms traditional metrics as it explicitly addresses 
their weaknesses -- it does not only support match on several levels -  exact, 
stem, synonym, and paraphrase but it also takes into account recall, 
distinguishes between functional and content words, allows language-specific 
tuning of parameters and many others.

It came with the Meteor Paraphrase tables \cite{meteor-tables}, which were
constructed automatically via \textit{pivoting}. \cite{pivoting} The pivot 
method is an inexpensive way of acquiring paraphrases from large parallel 
corpora. It is based on the assumption that two phrases that share a meaning 
may have a same translation in a foreign language. \cite{dyvik}

The basic setting of Meteor for evaluation of~Czech sentences offers two levels 
of matches -- exact and paraphrase. As we show further (see \Sref{parmesan}), 
its paraphrase tables are so noisy that they actually harm the performance of 
the metric, as it can award mistranslated and even untranslated words.


\section{Data}
\label{Data}
\subsection{Text Data}
We perform our experiments on data sets from the English-to-Czech translation 
task of WMT12 \cite{wmt12}, WMT13 \cite{wmt13} and WMT14 \cite{wmt14}. The data
sets contain 13/14\footnote{We use only 12 of them because two of them 
(FDA.2878 and online-G) have no human judgments.}/10 files with Czech outputs 
of MT systems.

In addition, each data set contains one file with corresponding reference 
sentences and one with original English source sentences. We perform 
morphological analysis and tagging of the hypotheses and the reference 
sentences using Morče \cite{morce:2007}.

The human judgment of hypotheses is available as a relative ranking of 
performance of five systems for~a~sentence. We calculated the sbsolute 
performance of every system by the “$ > $ others” method \cite{bojar-grains}, 
which was the WMT12 official system score. It is computed as 
$ \frac{wins}{wins+loses} $, ties among several systems are ignored. We use 
this score as a human judgment in further evaluation.
%We refer to this interpretation of human judgment as 
%\textit{silver standard} to distinguish it from the official system scores, which were 
%computed differently each year (here referred to as \textit{gold standard}).

\subsection{Sources of Paraphrases}
\label{meteori}
We make use of two existing sources of Czech paraphrases -- the Czech WordNet 
1.9 PDT \cite{czech-wordnet}, the Meteor Paraphrase Tables \cite{meteor-tables}. 
Czech PPDB \cite{ppdb} became available only recently and we plan to use it in 
further research.

The \textbf{Czech WordNet 1.9 PDT} is derived from the WordNet \cite{wordnet} 
by automatic translation followed by manual verification. It~contains rather 
high quality lemmatized paraphrases. However, their amount is~insufficient for 
our purposes - it contains only 13k pairs of synonymous lemmas. % and 
%- only one paraphrase per four sentences on~average is found in~the data. % (see \Tref{number_of_substitutions}).  

On~the other hand, Czech \textbf{Meteor Paraphrase tables} are quite the 
opposite of Czech WordNet -- they are large in size, but contain a lot of 
noise. The noise is particularly high among the multiword paraphrases -- for 
example: \textit{svého názoru} (its opinion) and \textit{šermovat rukama a 
mlátit neviditelného} (to flail one's arms and to beat the invisible one) are 
selected as a paraphrase. We used the multiword paraphrases only in our first 
experiment (see \Sref{lrec}). Using the substitution method paraphrasing, we 
show that the amount of noise in the multi-word paraphrases is so high that no 
automatic filtering method we used outperforms omitting them completely. (see
Appendix - \ref{multiword_filtering})
%Mozna je teda uplne vynechat i z toho lrecovskyho experimentu
 
Among one-word paraphrases the noise is sparser, but there are still pairs like 
\textit{1873} - \textit{pijavice} (a leech) or \textit{afgh\'{a}nci} (Afghans) -
\textit{š\v{t}astně} (happily) identified as synonyms. However, the biggest 
problem is that most of synonymous pairs were just different word forms of the 
same lemma. 

We performed automatically filtering among the one-word paraphrases in the 
Meteor table in the following way. Using Morče, we first perform morphological 
analysis of all one-word pairs and replace the word forms with their lemmas. We 
keep only pairs of different lemmas. Further, we dispose of pairs of words that 
differ in their parts of speech (POS)\footnote{With a single exception -- 
paraphrases consisting of numeral and corresponding digits, e.g., 
\textit{osmnáct} (eighteen) and \textit{18} - \textit{osmnáct} has the part of 
speech \textit{C}, which is designated for numerals, \textit{18} is marked with 
\textit{X} meaning it is an unknown word for the morphological analyzer.} or
~contain an unknown (typically foreign) word.

In this way we have reduced 684k paraphrases in~the original Czech Meteor 
Paraphrase tables to~only 32k pairs of lemmas. We refer to~this table as
~\textit{filtered Meteor}.

\section{Simple greedy substitution}
%tohle dat nekam na zacatek do introduction!! Tam to vsechno strucne zmini
%In this and following sections, we present our methods of paraphrasing reference sentences. 
%We start with simple greedy substitution similar to \cite{kauchak}. Then we used machine 
%translation systems themselves to create new references.
\label{lrec}
We experiment with several algorithms for paraphrasing reference sentences. 
This method is widely based on \cite{kauchak}. Our algorithm differ extends it 
with several methods for selecting potential paraphrase pairs and in the choice 
of paraphrases.

\subsection{Candidate Selection}
We select potential paraphrases using two different methods. The first one is a 
simple greedy search similar to~\newcite{kauchak}, the other one uses automatic 
word alignment for selecting corresponding segments of~the reference sentence 
and the hypothesis.

\subsection*{Simple Greedy Method}
Let $ r_1,..., r_n $,$ w_1,...,w_m $ be the hypothesis and the reference 
sentence, respectively. We performed their tagging and extracted sets of lemmas 
$ W_{L} $, $ R_{L} $. Then, one-word paraphrase candidates are chosen as:
$$ C_{L} = \lbrace (r,w) | r \in R_{L} \smallsetminus W_{L} \wedge w \in W_{L}
\smallsetminus R_{L}  \rbrace $$

Multi-words candidates $ C_M $ are selected as the Cartesian product of all 
sequences from the reference sentence and all sequences from the hypothesis. 
Maximum phrase length is seven words, because that is the length of~the longest
paraphrases in the data. % Formally:
%
%\begin{gather*}
%C_{M} = \lbrace (<r_i,..,r_{i+x}>, <w_j,...,w_{j+y}>) | \: 1 \leq i \leq n-x \: \wedge \\ 
% \: 1~\leq~j \leq m-y  \: \wedge \: 0 \leq x,y \leq 6 \: \wedge \: (x \neq 0 \vee y \neq 0) \rbrace 
%\end{gather*}

\subsection*{Word and Phrase Alignments}
One possible way to make the algorithm more reliable is to restrict the 
application of paraphrases to words/phrases which are aligned to each other. We 
compute word alignment between the reference translation and MT system outputs 
using GIZA++ \cite{gizapp}.

However, if we used only our test data to create the alignment (13 x 3003 + 12 
x 3000 = 75039 sentence pairs), the alignment quality would be insufficient. In 
order to make the training data for word alignment larger, we take advantage of 
the fact that all outputs are translations of the same data and also add all 
pairs of system outputs to our data, creating over 1,000,000 \equo{artificial} 
sentence pairs. For example, the parallel data for WMT12 then looks as follows:

\begin{center}
\begin{tabular}{ll}
Source & Target \\
\hline
system 1 & system 2 \\
system 1 & system 3 \\
... & ...\\
system 1 & system 13 \\
system 1 & reference \\
system 2 & system 1 \\
system 2 & system 3 \\
... & ... \\
system 13 & reference \\
\end{tabular}
\end{center}

We also experiment with adding much larger synthetic parallel data created by
machine translation (note that we need Czech-Czech data) but there was no 
impact on the quality of paraphrasing so we follow the outlined approach which 
requires no additional data or processing.

The set of~one-word candidates $C_L$ is then simply the set of all word pairs 
such that there exists an alignment link between them. The set $C_M$ is 
extracted using phrase extraction for phrase-based MT, the standard consistency 
criterion is~applied \cite{Och99improvedalignment}.

\subsection{Paraphrasing}
We reduce the set $ C_{L} $ to pairs appearing in our paraphrase tables in the 
following way. If a word appears in several synonymous pairs we give preference 
to those found in~WordNet or even better in the intersection of paraphrases 
from WordNet and filtered Meteor. Similarly, we filter $ C_{M} $ to pairs also 
contained in the multi-word Meteor tables.

We evaluate three different paraphrasing methods which differ in the order of
substitution.

\begin{description}
\item[One-word only] We proceed word by word from the beginning of the reference 
sentence to~its end. If a~lemma of~a~word appears as~the first member of~a~pair 
in reduced $ C_{L} $, it is replaced by~the word from hypothesis that has its lemma
as the second element of~that pair, i.e., paraphrase from the hypothesis. Otherwise, 
we keep the original word from the reference sentence.
\item[One-word first] We use \textit{One-word only} and then we apply longer paraphrases.
In that case we move ahead from the longest paraphrases to the shortest. That is because 
Meteor contains often even components of~phrases and we could substitute, instead of~whole 
phrase, only part of~it. We do not attempt to replace any word that was already changed 
before.
\item[Multi-word first] We substitute the longest confirmed paraphrases from
$ C_{M} $ and move to the shorter ones. We replace again only sequences that have not
been substituted yet. After this, we paraphrase the remaining unchanged words
with the \textit{One-word only} method.
\end{description}

\subsection{Depfix}
As we substitute a word form directly from a hypothesis, it may happen that a 
resulting new reference is not grammatically correct. To rectify this, we apply
Depfix \cite{depfix} -- an automatic post-editing system which is able to fix 
Czech sentences containing grammatical errors.

Depfix was originally designed for post-editing outputs of English-to-Czech 
phrase-based machine translation. It consists of a set of~linguistically-motivated 
rules and a statistical component that correct various kinds of errors, especially 
in grammar (e.g. morphological agreement), using a range of natural language 
processing tools to provide analyses of the input sentences.

We observe that the errors that appear in the outputs of our paraphrasing 
algorithm are often similar to some errors appearing in outputs of phrase-based
machine translation systems, e.g errors in morphological agreement are very 
common. This makes Depfix a good fit for fixing the errors, since typical 
grammar correcting tools, such as a grammar-checker in a word processor, focus
on errors that are typical for humans, not for machines. 

\subsection{Results}
Results of our method are presented in Tables \ref{corrs12} and \ref{corrs13}
as the Pearson correlation between human judgment and BLEU computed on our new
references. All evaluated approaches outperform the~baseline (i.e., using the 
original reference sentences), the simplest one \textit{One-word only} performs 
best (\Fref{example} shows an example of this method).

\begin{table*}[tb]
\begin{center}
\begin{tabular}{l|ccc|ccc}
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Greedy selection} & \multicolumn{3}{c}{Word alignment} \\
& No Depfix & Full Depfix & Limited Depfix & No Depfix & Full Depfix & Limited Depfix\\
\hline
One-word only     & 0.802 & 0.827 & \textbf{0.832} & 0.792 & 0.813 & 0.810 \\
One-word first    & 0.785 & 0.822 & 0.816 & 0.767 & 0.792 & 0.798 \\
Multi-word first & 0.768 & 0.810 & 0.804 & 0.761 & 0.781 & 0.778 \\
\end{tabular}

\vspace{10pt}

Baseline~correlation: \textbf{0.749}
\caption{Correlation of the human judgment and BLEU computed with the data from WMT12}
\label{corrs12}
\end{center}
\end{table*}

\begin{table*}[tb]
\begin{center}
\begin{tabular}{l|ccc|ccc}
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Greedy selection} & \multicolumn{3}{c}{Word alignment} \\
& No Depfix & Full Depfix & Limited Depfix & No Depfix & Full Depfix & Limited Depfix\\
\hline
One-word only     & 0.861 & \textbf{0.887} & 0.883 & 0.856 & 0.877 & 0.872 \\
One-word first    & 0.851 & 0.880 & 0.875 & 0.833 & 0.871 & 0.863 \\
Multi-word first  & 0.838 & 0.870 & 0.864 & 0.833 & 0.868 & 0.861 \\
\end{tabular}

\vspace{10pt}

Baseline~correlation: \textbf{0.829}
\caption{Correlation of the human judgment and BLEU computed with the data from WMT13.}
\label{corrs13}
\end{center}
\end{table*}
 
We use a freely available implementation\footurl{http://www.cnts.ua.ac.be/~vincent/scripts/rtest.py} of \newcite{meng:1992} to determine whether the difference in correlation
coefficients is statistically significant. The test shows that BLEU performs
better with our reference sentences with 99\% certainty. 

\begin{figure}[tb]
\begin{center}
\scalebox{0.89}{
\begin{tabular}{ll}
 Source &  \begin{tabular}{l}
  	\textit{The location alone is classic.} \\
	\end{tabular} \\
 \hline
 
 Hypothesis & \begin{tabular}{llll}
 			\textit{Samotné} & \textit{místo} & \textit{je} & \textit{klasické.} \\
 			Actual & place & is & classic \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	The place alone is classic. \\
	\end{tabular} \\

 \hline
 %\noindent\rule{8cm}{0.4pt}\\
 Reference & \begin{tabular}{llll}
 			\textit{Už} & \textit{poloha} & \textit{je} & \textit{klasická.} \\
 			Already & position & is & classic. \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	The position itself is classic. \\
	\end{tabular}  \\ 

 \hline
  New ref. & \begin{tabular}{llll}
 			\textit{Už} & \textit{místo} & \textit{je} & \textit{klasická.} \\
 			Already & place & is & classic \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	*The place itself is classic. \\
	\end{tabular} \\
 \hline 
  Depfixed ref. & \begin{tabular}{llll}
 			\textit{Už} & \textit{místo} & \textit{je} & \textit{klasické.} \\
 			Already & place & is & classic \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	The place itself is classic. \\
	\end{tabular} \\
 \hline  
 
\end{tabular}
}
\caption{Example of the \textit{One-word only} method. The~hypothesis is 
grammatically correct and has very similar meaning as the reference sentence. 
The new reference is closer in wording to the hypothesis, but there is no 
agreement between the noun and adjective. Depfix resolves the error and the 
final reference is correct and much similar to~the hypothesis.}
\label{example}
\end{center}
\end{figure}

Multi-word paraphrases are very noisy and while they do bring the system 
outputs closer to the reference (the average BLEU score of the systems 
increases), they often propose non-equivalent translations or violate the 
correctness of the sentence, thus blurring the differences between systems.

When paraphrasing is restricted by word alignment, all methods perform worse. 
As Table \ref{replaced} shows, the number of applied paraphrases is much lower: 
while the proportion of correct paraphrases is higher, their amount is reduced 
too much and overall, our technique is harmed by this restriction. 

\begin{table}[htb]
\begin{center}
\scalebox{0.85}{
\begin{tabular}{l|cc|cc}
\multicolumn{5}{c}{\textbf{WMT12}}\\
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Greedy selection} & \multicolumn{2}{c}{Word alignment} \\
& Words & Phrases & Words & Phrases \\
\hline
One-word only     & 1.59 & --   & 0.86 &  --  \\
One-word first    & 1.59 & 0.23 & 0.86 & 0.22 \\
Multi-word first  & 1.38 & 0.31 & 0.81 & 0.27 \\
\end{tabular}}
\vspace{10pt}

\scalebox{0.85}{
\begin{tabular}{l|cc|cc}
\multicolumn{5}{c}{\textbf{WMT13}}\\
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Greedy selection} & \multicolumn{2}{c}{Word alignment} \\
& Words & Phrases & Words & Phrases \\
\hline
One-word only    & 1.33 &  --  & 0.76 & --   \\
One-word first   & 1.33 & 0.20 & 0.76 & 0.20 \\
Multi-word first & 1.04 & 0.68 & 0.74 & 0.24 \\
\end{tabular}}
\caption{Average number of replaced words/phrases.}
\label{replaced}
\end{center}
\end{table}

On the other hand, applying Depfix is always beneficial, with the positive 
effects ranging from 0.017 up to 0.042. This supports our assumption of the 
importance of grammatical correctness of the created references. However, the 
\textit{limited} version is not optimally chosen and performs worse than the 
\textit{full} version in most cases.

Results on the data from WMT13 and WMT12 are very similar. Again, paraphrasing 
helps to increase the accuracy of the evaluation, even though the differences 
on the WMT13 data are not as~big due to much higher baseline. This is also 
reflected in~the smaller amount of substitutions (see \Tref{replaced13}).

\subsection{Parmesan}
\label{parmesan}
Based on the positive impact of filtering Meteor Paraphrase Tables for targeted
lexical paraphrasing of reference sentences, we experiment with the filtering 
them yet again, but this time as~an~inner part of~the Meteor evaluation metric 
(i.e. for the paraphrase match).

\begin{table*}[htb]
\begin{center}

\scalebox{0.99}{
\begin{tabular}{r|c|l}
setting & size & description of the paraphrase table \\
\hline
\textbf{Basic} & 684k & The original Meteor Paraphrase Tables \\
\textbf{One-word} & 181k & \textbf{Basic} without multi-word pairs\\
\textbf{Same POS} & 122k & \textbf{One-word} + only same part-of-speech pairs\\
\textbf{Diff. Lemma} & 71k & \textbf{Same POS} + only forms of different lemma\\
\textbf{Same Lemma} & 51k & \textbf{Same POS} + only forms of same lemma\\
\textbf{No paraphr.} & 0 & No paraphrase tables, i.e., exact match only\\
\textbf{WordNet} & 202k & Paraphrase tables generated from Czech WordNet\\
\end{tabular}
}
\caption{Different paraphrase tables for Meteor and their size (number of paraphrase pairs).}
\label{meteory}
\end{center}
\end{table*}

The filtering of~the paraphrase tables is performed analogically. We experiment
with seven different settings that are presented in~\Tref{meteory}. All of them
are created by reducing the original Meteor Paraphrase tables, except for the 
setting referred to~as~\textbf{WordNet} in the table. In this case, the 
paraphrase table is generated from one-word paraphrases in Czech WordNet to all
their possible word forms found in~CzEng \cite{czeng}.

Prior paraphrasing reference sentences and using Meteor with the \textbf{No 
paraphr.} setting for computing scores constitutes Parmesan -- our submission
to the WMT14 \cite{wmt14} for evaluation English-to-Czech translation. In the 
tables with results, Parmesan scores are highlighted by the box and the best 
scores are in bold. % \xxx{podle lopatkovy preformulovat}

\begin{table*}[htb]
\begin{center}
\scalebox{0.95}{
\begin{tabular}{l|ccccccc}
\multicolumn{8}{c}{\textbf{WMT12}}\\
\hline
reference & Basic & One-word & Same POS & Same Lemma & Diff. Lemma & No paraphr. & WordNet \\
\hline
Original & 0.833 & 0.836 & 0.840 & 0.838 & 0.863 & 0.861 & 0.863 \\
Before Depfix & 0.905 & 0.908 & 0.911 & 0.911 & 0.931 & 0.931 & 0.931 \\
New & 0.927 & 0.930 & 0.931 & 0.932 & 0.950 & \boxed{\textbf{0.951}} & \textbf{0.951} \\
%\hline
\end{tabular}} 

\vspace{10pt}

\scalebox{0.95}{
\begin{tabular}{l|ccccccc}
\multicolumn{8}{c}{\textbf{WMT13}}\\
\hline
references & Basic & One-word & Same POS & Same Lemma & Diff. Lemma & No paraphr. & WordNet \\
\hline
Original & 0.817 & 0.820 & 0.823 & 0.821 & 0.850 & 0.848 & 0.850 \\
Before Depfix  & 0.865 & 0.867 & 0.869 & 0.868 & 0.895 & 0.895 & 0.894 \\
New  & 0.891 & 0.892 & 0.893 & 0.892 & \textbf{0.915} & \boxed{\textbf{0.915}} & \textbf{0.915} \\
%\hline
\end{tabular}
}
\caption{Pearson's correlation of Meteor and the human judgment.}
\label{results:12:13}
\end{center}
\end{table*}

The results of our experiments are presented in \Tref{results:12:13}. The 
results for WMT12 and WMT13 are very consistent. There is a clear positive 
impact of~the prior paraphrasing of~the reference sentences and of applying 
Depfix. The results also show that independently of~a~reference sentence used, 
reducing the Meteor paraphrase tables in evaluation is always beneficial.

%We use a~freely available implementation \footnote{//www.cnts.ua.ac.be/$ \thicksim $vincent/scripts/rtest.py} 
%of~\newcite{meng:1992} to determine whether the difference in~correlation 
%coefficients is statistically significant. The tests show that Parmesan performs better than
%original Meteor with 99\% certainty on the data from WMT12 and WMT13.

\textbf{Diff. Lemma} and \textbf{WordNet} settings give the best results on the 
original reference sentences. That is because they are basically a limited 
version of the paraphrase tables we use for creating our new references, which 
contain both all different lemmas of the same part of speech from Meteor 
Paraphrase tables and all lemmas from the WordNet.

The main reason of the worse performance of the metric when employing the 
Meteor Paraphrase tables is the noise. The metric may award even parts of the 
hypothesis left untranslated, as the original Meteor Paraphrase tables contain 
some English words and their Czech translations as paraphrases. There are for
~example pairs: \textit{pšenice} - \textit{wheat}\footnote{In all examples the 
Czech word is the correct translation of the English side.}, \textit{vůdce} - 
\textit{leader}, \textit{vařit} -	\textit{cook}, \textit{poloostrov} - 
\textit{peninsula}. For these reasons, the differences among the systems are 
more blurred and the metric performs worse than without using the paraphrases. 

\section{Paraphrasing using Phrase based Machine Translation}
\label{MT}
While this simple method offers good result, it is very limited. We would like
to be able to use longer paraphrases, word order changes, switch between active 
and pasive construction, etc.

For this purpose, we employ machine translation itself as there is a close 
resemblance between translation and paraphrasing. They both attempt to preserve 
the meaning of a sentence, the first one between two languages and the second 
one within one language by different word choice. %\cite{madnani:2010} 

However, there are many more tools for MT than for paraphrasing. Therefore, it 
seems only natural to attempt to adjust some MT tools to translate within a 
single language for targeted paraphrasing. 

We describe this attempt on two types of MT systems -- phrase-based and 
rule-based. Initially, we experiment with the freely available SMT system 
Moses.\cite{moses} We create translation from Czech WordNet and the Meteor 
Paraphrase tables. We extended Moses by a new feature that makes the 
translation targeted. 

However, the results of this method are inconclusive. In the view of errors 
appearing in the new paraphrased sentences, we propose another solution -- 
targeted paraphrasing using parts of a rule-based translation system included 
in the NLP framework Treex.

\subsection{Paraphrasing using Moses}
Moses is a freely available statistical machine translation engine. In a 
nutshell, statistical machine translation involves the following phases: 
creating language and translation models, parameter tuning and decoding. We use 
Moses in the phrase-based setting.

A language model is responsible for a correct word order and grammatical 
correctness of the translated sentence. A translation model (phrase table) 
supplies all possible translations of a word or a phrase. Models are assigned 
weights which are learned during the parameter tuning phase.

During the decoding phase, all these models are combined to maximize 
$ \sum_i \lambda_i \phi_i (\bar{f},\bar{e}) $, where  $ \lambda_i $ is a weight 
of a the sub-model $ \phi_i $ and $ \bar{f},\bar{e} $ is a hypothesis and 
source sentence, respectively. In our case, we want to make a reference 
sentence closer to a corresponding machine translation output -- $ \bar{e} $ is 
the reference sentence and $ \bar{f} $ is a new synthetic reference.

On its own, this setting could create paraphrases, but they would be just 
random paraphrases of the reference sentence -- their similarity in wording to 
our original hypotheses would not be guaranteed. Therefore, we also add a new 
feature for targeted paraphrasing to Moses.

\subsubsection{Language model}
We create the language model (LM) using the SRILM toolkit \cite{srilm} on the 
data from the Czech part of the Czech-English parallel corpus CzEng. 

\subsubsection{Phrase models}
Each entry in Moses phrase tables contains a phrase, its translation, several
feature scores (translation probability, lexical weight etc.), and optionally
also alignment within the phrase and frequencies of phrases in the training 
data. The phrase tables are learned automatically from large parallel data.
As we do not have any large corpora of Czech-Czech parallel data, we create the 
following two ``fake'' translation models for paraphrasing from our paraphrase 
tables. 

\begin{itemize}
\item \textbf{Enhanced Meteor tables}\\
This table was created from the Czech Paraphrase Meteor tables. Each paraphrase 
pair comes with a pivoting score which we adapt as a feature in out phrase 
table. Further, we add our own paraphrase scores, acquired by 
\textit{distributional semantics}. Distributional semantics assumes that two 
phrases are semantically similar if their contextual representations are 
similar. \cite{miller}

We collect all contexts (words in a window of limited size) in which Meteor 
paraphrases occur in the Czech National Corpus \cite{SYN2010} and then measure 
context similarity cosine distance, taking into account the number of word 
occurrences) for each pair of paraphrases. 

We add six scores for each pair of paraphrases according to the size of the 
context window used (1-3 words) and whether word order played a role in the 
context. 

\item\textbf{One-word paraphrase table}\\
We first create a set of all words from Czech side of CzEng appearing at 
least five times to exclude rare words and possible typos. We also add all 
words appearing in the MT outputs and the reference sentences. Morphological 
analysis of the words was then performed using Morče. 

For every word $ x $ from this set, we add to this translation table every pair 
of words that fulfils at least on of the following requirements:

\begin{itemize}
\item $ x,x $ (not every word should be paraphrased)
\item $ x,y $, if lemma of $ x $ is lemma of $ y $ (some word 
might have different morphology in the paraphrased sentence)
\item $ x,y $, if lemma of $ x $ and lemma of $ y $ are paraphrases according 
to Czech WordNet PDT 1.9.
\item $ x,y $, if lemma of $ x $ and lemma of $ y $ are paraphrases according 
to the filtered Meteor.
\end{itemize}

These categories constitute the first four scores in the phrase table. A pair 
of words gets score $ e $ if they fall in a given category, 1 ($e^0$) 
otherwise.\footnote{Phrase-table scores are considered log-probabilities.} 
This phrase table contains more than 1,100k pairs of words.

We add another score expressing POS tag similarity between the two words. It is 
computed $ e^{\frac{1}{a+1}}$, where $ a $ is the minimal Hamming distance 
between tags of the words. This probability should reflect how morphologically 
distant the paraphrases are. 
\end{itemize}

\subsubsection{Feature for targeted paraphrasing}
In order to steer the MT decoder (translation engine) in the direction of the 
hypotheses, we implemented an additional feature for Moses which measures the 
overlap with the hypothesis. In order to keep its computation tractable during 
search, the overlap is defined simply as the number of words from the 
hypothesis confirmed by the reference translation.

Integration into the beam search algorithm used in phrase-based decoding
requires us to keep track of feature state (i.e. reference words covered) to
allow for correct hypothesis recombination. We also implemented an estimator of
future phrase score, defined as the number of reference translation words
covered by the given phrase. Our code is included in
Moses.\footurl{https://github.com/moses-smt/mosesdecoder/}

\subsubsection{Parameter tuning}
We use the minimum error rate training (MERT) \cite{mert} to find the optimal 
weights for our models. MERT asserts the weights to maximize the translation 
quality, which is measured with BLEU. We employ the reference sentences and the 
highest rated MT output as the parallel data for tuning. 

This method, however, turned out not to be optimal for our setting. Our feature 
for targeted paraphrasing naturally obtains the highest weight as it provides 
an oracle guide towards the hypothesis.

Other important models, e.g. the language model, get comparably very small 
weights. The paraphrased sentences tend to be closer to the hypothesis, but not 
grammatically correct. Therefore, we experiment with increasing the weight of 
the language model manually. 

\subsubsection{Results}
We compare four different basic settings, the results are presented in 
\Tref{settings} as the Pearson’s correlation coefficient of BLEU and the human 
judgment. In contrast to  our previous results, the baseline score is not 
exceeded by any of our paraphrasing methods.
%A visualization of the results is shown in \Fref{visualization}. 

\begin{table*}[ht]
%\begin{center}
\begin{tabular}{r|l|c|c}
setting & reference sentence used & correlation & avg. BLEU \\
\hline
\textbf{Baseline} & original reference sentence, no paraphrasing & \textbf{0.75} & 12.8 \\
\textbf{Paraphrased} & paraphrased by Moses using MERT-learned weights  & 0.50  & 15.8 \\
\textbf{LM+0.2}  & paraphrased by Moses with LM weight increased by 0.2  & 0.24 & 9.1 \\
\textbf{LM+0.4} & paraphrased by Moses with LM weight increased by 0.4  & 0.22 & 6.7 \\
\end{tabular}
\caption{Description of basic settings and the results - Pearson's correlation of BLEU and the
human judgment, the average BLEU scores.} 
%korelace metrik - 0.981
\label{settings}
%\end{center}
\end{table*}

There are several reasons for the clear decrease in correlation with 
paraphrased references. Hypotheses generated by the \textbf{Paraphrased} 
setting, while obtaining a significantly higher BLEU score, were mostly 
ungrammatical and reduced the correlation of our metric.

The small weight of the language model seems to be the problem, but its 
increase brings even more chaos. It creates hypotheses which are nice and 
grammatically correct but often wholly unrelated to the source sentence.

This shows that our paraphrase table noise filtering was by no means sufficient 
and there is still a lot of noise in our phrase tables. Furthermore, the MT 
output might be far from being a correct sentence -- given the high weight for 
the targeted paraphrase feature, we essentially transform the correct reference 
sentences to incorrect hypotheses at all cost, using our noisy phrase tables.

Our targeting feature is also not ideal -- it ignores word order and operates
only on the word level (it does not model phrases). Ungrammatical translations
with scrambled word order are considered perfectly fine so long as the
translation contains the same words as the reference. So while the feature does
provide a kind of oracle, it does not guarantee reaching the best possible
translation in terms of BLEU score, let alone a grammatical translation.

Another problem is illustrated by very small weights assigned to our 
translation models. In fact, the highest weight was assigned to the tag 
similarity feature. This shows that our model features (Meteor score and 
distributional similarity scores) fail to distinguish good paraphrases from the 
noise. 

The combination of noise in the translation tables and the boosted language
model then caused that during the decoding phase, the most common paraphrase
according to the language model with a similar tag got the preference. 

\Fref{example} represents an example of our paraphrasing method. The~hypothesis 
is grammatically correct and has a very similar meaning as the reference 
sentence. The new paraphrased reference is slightly closer in wording to the 
hypothesis, but there is an error due to a bad word choice. The boosted 
language model reduces errors, however the meaning of the sentences is shifted. 
In the \textbf{LM+0.4} setting, they also differ a lot in wording from both the 
hypothesis and the reference sentence.

\begin{figure*}[htb]
\begin{center}
\begin{tabular}{l|r}
 \textbf{Source } &  \textit{Paclík claims he would dare to manage the association.} \\
 \hline
 
 \textbf{Baseline} & Paclík tvrdí , že by si na vedení asociace troufl.\\
           & \textit{Paclík claims he would dare to lead the association.} \\

 \hline
 \textbf{Hypothesis} & Paclík tvrdí, že by se odvážil k řízení komory. \\
            & \textit{Paclík claims he would find the courage to control the chamber.}  \\ 

 \hline
 \textbf{Paraphrased} & Paclík tvrdí, že by se na řízení organizace troufl. \\
             & \textit{*Paclík claims he would dare to control the organization.}\\
 \hline 
 \textbf{LM+0.2} & Paclík tvrdí, že by si troufl na řízení ekonomiky. \\
               &\textit{ Paclík claims he would dare to control the economy.} \\
  
\hline 
 \textbf{LM+0.4} & Říká se, že Paclík si troufl na řídící rady. \\
               & \textit{They say that Paclík ventured to governing boards.} \\
%Lexical & Paclík tvrdí , že by se na řízení asociace troufl .\\
%Lexical boosted by 20 & Paclík tvrdí , že by si troufl na řízení ekonomiky .\\
%asociace, komora nikde nejsou jako parafraze

\end{tabular}
\caption{Example of the targeted paraphrasing. The~hypothesis is grammatically 
correct and has very similar meaning as the source sentence. The new reference 
is closer in wording to the hypothesis, but there is an error in a word choice. 
The sentences created with increased weights of the language model are both 
grammatically correct, but the sentence lost its original meaning.}
\label{example}
\end{center}
\end{figure*}

\subsection{Rule based Machine Translation}
Based on previous results, Moses does not seem to be the optimal tool for our 
task, especially unless we have at our disposal better paraphrasing tables, or
we develop better targeting function. %TODO: lepsi tuning? 

Furthermore, there may be a better solution than a phrase-based translation
system, namely Treex \cite{treex}, a highly modular NLP software system. Treex
was developed for TectoMT, which is a rule-based machine translation system 
that operates on deep syntactic layer.

Treex implements the stratificational approach to language, adopted from the 
Functional Generative Description theory \cite{FGP} and its later extension by 
the Prague Dependency Treebank \cite{PDT3.0}. It represents sentences in four 
layers: word layer, morphological layer, shallow-syntax layer and deep-syntax 
layer (tectogrammatical layer).

Using Treex, we can easily overcome some of the problems that appear when 
paraphrasing using Moses. First of all, we only compare two sentences and there 
is no need to create paraphrase tables, thus less space for the noise to 
interfere. Also there already is highly developed machinery to avoid 
ungrammatical sentences. We can change only parts of sentences that are 
dependent on the changed word, thus keeping the rest of the sentence correct 
and creating more conservative sentences.

We performed simple paraphrasing using Treex and started with implement the
reordering module.

\subsection{Paraphrasing}
We transfer both hypothesis and reference sentence to the tectogrammatical 
layer, where we extract lemmas that appear in only one of the sentences. We 
filter those based on WordNet and filtered Meteor.

Into the tectogramatical tree of a reference sentence we substituted the 
Furthermore, we are able to transfer a reference sentence to a tectogrammatical 
layer, where we can replace individual lemmas from the hypothesis with their 
paraphrases and corresponding grammatemes. Then we transfer the altered reference sentence back to 
the word layer.

This way should easily overcome some of the problems that appear when paraphrasing using 
Moses. First of all, we only compare two sentences and there is less space for %neni treba stavet tabulky
the noise to interfere. Also there is highly developed machinery to avoid ungrammatical 
sentences. We can change only parts of sentences that are dependent on the changed 
word, thus keeping the rest of the sentence correct and creating more conservative 


Pohled na možnost volby se několikrát změnil, ale neuralgický bod je stále stejný.
Názor volebních možností změnil několikrát, ale klíčový bod zůstává stejný.
Názor na možnost volby se změnil několikrát, ale neuralgický bod zůstává stále stejný.

Jenže teď už čekat nehodlá a do dalšího života stanovila jasná pravidla.
Nicméně nechce čekat a nastavit jasná pravidla pro svůj budoucí život.
Jenže teď už nechce čekat a nastavila jasná pravidla do dalšího života


\section{Future Work}
We would like to work to enhance Treex paraphrases, next step could be multiword 
paraphrases, better word reordering, deleting empty word (stuffing?], pekny by byly
taky nejaky ty hierarchicke pravidla. 

Ze ciste zvedavosti bych to taky chtela aplikovat na dalsi jazyky pro ktere ma treex
analyzu a syntezu, AJ, ES...

\bibliographystyle{acl}
\bibliography{biblio}

\section*{Appendix}
\subsection*{Automatic Filtering of Meteor tables}
\label{multiword_filtering}
Filtering strategies described in~this section are based on assigning a score 
to each paraphrase pair. We then gradually remove paraphrases with low scores 
and measure the effect on~the final correlation of~our metric.

The first, straightforward approach is to use the paraphrase scores already
provided in Meteor. They are based on~phrasal translation probabilities and it 
corresponds to~paraphrase probability in~the pivoting model.

We propose an alternative scoring based on pivoting and lexical translation
scores:

$$\text{lex\_p}(\mathbf{s},\mathbf{t}) = \sum_{s \in \mathbf{s}}\sum_{t \in
\mathbf{t}}\sum_{pivot}\text{lex}(s|pivot)\text{lex}(pivot|t)$$

In this case, pivots are all words aligned to both $s$ and $t$ in the parallel
data. To get lexical translation probabilities, we use maximum likelihood
estimation from single best word alignment computed on CzEng 1.0
\cite{czeng10:lrec2012}. We refer to this score as \emph{lexical pivoting}.

We use random selection as the baseline -- paraphrases are simply shuffled and 
we then use the first 10, 20,$\ldots$ percent of them.

We only evaluate the filtering techniques on the WMT12 data. First, we attempt
to filter one-word paraphrases and use the cleaner paraphrase table in the
\emph{one-word-only} paraphrasing strategy. Note that our paraphrase table has
already been filtered using the error-analysis based filtering described in 
\Sref{Data}.

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.55]{filtering-lexical-cropped.pdf}
\caption{Comparison of automatic filtering techniques for~\emph{one-word} 
paraphrases on WMT12 data.} 
\label{fig:filtering-lexical}
\end{center}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.55]{filtering-mwe-cropped.pdf}
\caption{Automatic filtering of multi-word paraphrase for~the 
\textit{multi-word-first} scenario on WMT12 data.}
\label{fig:filtering-mwe}
\end{center}
\end{figure}


\Fref{fig:filtering-lexical} shows the performance of different filtering
techniques for one-word paraphrases. Relying on Meteor scores proves worse than
random selection. Using lexical pivoting, we can keep a high correlation even 
if we throw away as much as 90\% of the paraphrases, however we do not improve 
(by~a~relevant margin) upon the baseline correlation of 0.802 achieved by
\emph{one-word-only} paraphrasing with the full paraphrase table.

We evaluate the best-performing technique also in the \textit{multi-word-first}
scenario where we use it for filtering multi-word paraphrases (see
\Fref{fig:filtering-mwe}). As we reduce the number of paraphrases, we observe a
considerable improvement of~correlation, however we never outperform
\textit{one-word-only} or \textit{one-word-first}. In this case, the filtering
simply mitigates the damage done by the multi-word paraphrases. We
cannot hope to achieve a higher score without a~more fine-grained grip on what
a~good multi-word paraphrase is.

\end{document}
